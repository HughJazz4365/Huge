const std = @import("std");
const zigbuiltin = @import("builtin");
const huge = @import("../../root.zig");
const math = huge.math;
const util = huge.util;
const gpu = huge.gpu;
const hgsl = gpu.hgsl;

const frmt = @import("format.zig");
//generated by: https://github.com/Snektron/vulkan-zig
pub const vk = @import("vk.zig");

//=====|constants|======
//2x2 image: |purple| green|
//           |red   | blue |
const default_image = [_]u8{
    156, 39,  176, 255,
    0,   244, 92,  255,
    255, 0,   0,   255,
    3,   81,  244, 255,
};

const u32m = ~@as(u32, 0);
const timeout: u64 = std.time.ns_per_s * 5;
const max_queue_family_count = 16;

const min_vulkan_version: Version = .{ .major = 1, .minor = 2 };
const max_vulkan_version: Version = undefined;

const max_push_constant_bytes = 128;
const layers: []const [*:0]const u8 = if (zigbuiltin.mode == .Debug) &.{
    "VK_LAYER_KHRONOS_validation",
    // "VK_LAYER_LUNARG_api_dump",
} else &.{};

//=======|state|========

var vka: ?*vk.AllocationCallbacks = null;

var instance: vk.InstanceProxy = undefined;
var physical_devices: [3]PhysicalDevice = @splat(.{});
var valid_physical_device_count: usize = 0;
var current_physical_device_index: usize = 0;
var device: vk.DeviceProxy = undefined;

var bwp: vk.BaseWrapper = undefined;
var iwp: vk.InstanceWrapper = undefined;
var dwp: vk.DeviceWrapper = undefined;
var queues: [queue_type_count]vk.Queue = @splat(.null_handle);
inline fn queue(queue_type: QueueType) vk.Queue {
    return queues[@intFromEnum(queue_type)];
}

var command_pools: [queue_type_count]vk.CommandPool = @splat(.null_handle);

inline fn pd() PhysicalDevice {
    return physical_devices[current_physical_device_index];
}
var physical_device_memory_properties: vk.PhysicalDeviceMemoryProperties = undefined;

var api_version: Version = undefined;
var arena: std.heap.ArenaAllocator = undefined;
var shader_compiler: hgsl.Compiler = undefined;
// var pipelines: List(VKPipeline) = .empty;

var shader_module_list: List(VKShaderModule) = .empty;
var pipeline_list: List(VKPipeline) = .empty;
var buffer_list: List(VKBuffer) = .empty;
var texture_list: List(VKTexture) = .empty;
var memory_allocation_list: List(MemoryAllocation) = .empty;
var render_target_list: List(VKRenderTarget) = .empty;

var window_context_primary: VKWindowContext = .{};
var window_context_list: List(VKWindowContext) = .empty;
var window_context_count: u32 = 0;

const null_render_target: RenderTarget = @enumFromInt(~@as(gpu.Handle, 0));
var current_render_target: RenderTarget = null_render_target;
const MemoryAllocation = struct {
    memory: vk.DeviceMemory = .null_handle,
    ref_count: gpu.Handle = 0,
    pub fn free(self: *MemoryAllocation) void {
        if (self.memory != .null_handle)
            device.freeMemory(self.memory, vka);
        self.memory = .null_handle;
    }
};
//======|methods|========

fn draw(handle: Pipeline, params: gpu.DrawParams) Error!void {
    const cmd = try getRenderingResources();
    if (cmd == .null_handle) return;

    errdefer forceEndRendering();

    device.cmdBindPipeline(cmd, .graphics, VKPipeline.get(handle).vk_handle);
    if (params.indexed_vertex_offset) |vo| {
        device.cmdDrawIndexed(
            cmd,
            params.count,
            params.instance_count,
            params.offset,
            vo,
            params.instance_offset,
        );
    } else device.cmdDraw(
        cmd,
        params.count,
        params.instance_count,
        params.offset,
        params.instance_offset,
    );
}
fn bindIndexBuffer(handle: Buffer, index_type: gpu.IndexType) Error!void {
    const cmd = try getRenderingResources();
    if (cmd == .null_handle) return;
    errdefer forceEndRendering();

    const buffer: *VKBuffer = .get(handle);
    if (buffer.usage != .index) return Error.BufferMisuse;

    if (api_version.minor < 4 and index_type == .u8)
        @panic("TODO: index_type u8 extension");
    device.cmdBindIndexBuffer(cmd, buffer.vk_handle, 0, switch (index_type) {
        .u32 => .uint32,
        .u16 => .uint16,
        .u8 => .uint8, //only in core spec from 1_4
    });
}
fn bindVertexBuffer(handle: Buffer) Error!void {
    const cmd = try getRenderingResources();
    if (cmd == .null_handle) return;
    errdefer forceEndRendering();

    const buffer: *VKBuffer = .get(handle);
    if (buffer.usage != .vertex) return Error.BufferMisuse;

    device.cmdBindVertexBuffers(cmd, 0, 1, &.{buffer.vk_handle}, &.{0});
}
fn beginRendering(render_target: RenderTarget, clear_value: ClearValue) Error!void {
    try endRendering();

    current_render_target = render_target;
    if (VKWindowContext.fromRenderTarget(render_target)) |wc| {
        try wc.setRenderTargetState(clear_value);
    } else @panic("begin rendering NOT to WINDOW");
}
fn endRendering() Error!void {
    if (current_render_target == null_render_target) return;
    defer current_render_target = null_render_target;

    if (VKWindowContext.fromRenderTarget(current_render_target)) |wc| {
        try wc.endRendering();
    } else @panic("end rendering NOT to WINDOW");
}

fn forceEndRendering() void {
    if (current_render_target == null_render_target) return;
    defer current_render_target = null_render_target;

    if (VKWindowContext.fromRenderTarget(current_render_target)) |wc| {
        wc.forceEndRendering();
    } else @panic("end rendering URGENT NOT to WINDOW");
}

fn reloadPipelines() Error!void {
    device.deviceWaitIdle() catch return;
    for (shader_module_list.items) |*sm| sm.load_state = .unloaded;
    for (pipeline_list.items) |*p| {
        for (p.shader_modules[0..p.stage_count]) |s| {
            VKShaderModule.get(s).recreate();
        }
        try p.recreate();
    }

    forceEndRendering();
}
// setPipelineOpaqueUniform: SetPipelineOpaqueUniformFn = undefined,
fn pipelinePushConstant(
    handle: Pipeline,
    name: []const u8,
    local_offset: u32,
    local_size: u32,
    ptr: *const anyopaque,
) Error!void {
    _ = .{ local_offset, local_size };
    const cmd = try getRenderingResources();
    if (cmd == .null_handle) return;

    const pipeline: *VKPipeline = .get(handle);

    var offsets: [Pipeline.max_pipeline_stages]u32 = undefined;
    var offset_count: u32 = 0;
    for (pipeline.entry_point_info_storage[0..pipeline.stage_count]) |ep_info| {
        pcloop: for (ep_info.push_constant_mappings) |pc| {
            if (util.strEql(name, pc.name)) {
                if (pc.offset + pc.size > max_push_constant_bytes)
                    return Error.ShaderPushConstantOutOfBounds;
                //dont repeat cmdPushConstants call with the same offset
                for (offsets[0..offset_count]) |o| if (o == pc.offset) continue :pcloop;

                offsets[offset_count] = pc.offset;
                offset_count += 1;

                var stage_flags: vk.Flags = 0;
                for (pipeline.push_constant_ranges[0..pipeline.push_constant_range_count]) |r| {
                    if (r.size < pc.offset + pc.size) break;
                    stage_flags |= r.stage_flags.toInt();
                }

                device.cmdPushConstants(
                    cmd,
                    pipeline.layout,
                    @bitCast(stage_flags),
                    pc.offset,
                    pc.size,
                    ptr,
                );
            }
        }
        //log if didnt found
    }
}
fn createPipeline(stage_modules: []const ShaderModule, params: gpu.PipelineParams) Error!Pipeline {
    const handle: Pipeline = @enumFromInt(@as(gpu.Handle, @intCast(pipeline_list.items.len)));
    try pipeline_list.append(arena.allocator(), try VKPipeline.create(stage_modules, params));
    return handle;
}

fn createShaderModulePath(path: []const u8, entry_point: []const u8) Error!ShaderModule {
    const handle: ShaderModule = @enumFromInt(@as(gpu.Handle, @intCast(shader_module_list.items.len)));
    try shader_module_list.append(arena.allocator(), try VKShaderModule.createPath(path, entry_point));
    return handle;
}
fn destroyShaderModule(handle: ShaderModule) void {
    VKShaderModule.get(handle).destroy();
}
fn loadBuffer(handle: Buffer, bytes: []const u8, offset: usize) Error!void {
    const mapped = try mapBuffer(handle, bytes.len, offset);
    @memcpy(mapped, bytes);
    defer unmapBuffer(handle);
}
fn mapBuffer(handle: Buffer, bytes: usize, offset: usize) Error![]u8 {
    const buffer: *VKBuffer = .get(handle);
    if (buffer.mapped) return Error.MemoryRemap;

    if (bytes > buffer.size) return Error.OutOfMemory;
    const ptr = (device.mapMemory(accessMemory(buffer.device_memory), offset, @intCast(bytes), .{}) catch
        return Error.OutOfMemory) orelse
        return Error.OutOfMemory;
    buffer.mapped = true;
    return @as([*]u8, @ptrCast(ptr))[0..bytes];
}
fn unmapBuffer(handle: Buffer) void {
    const buffer: *VKBuffer = .get(handle);
    if (!buffer.mapped) return;

    device.unmapMemory(accessMemory(buffer.device_memory));
    buffer.mapped = false;
}
fn createRenderTargetFromTextures(color: ?Texture, depth_stencil: ?Texture) Error!RenderTarget {
    var rt: VKRenderTarget = .{};
    if (color) |c| {
        rt.mask.color = true;
        rt.color_attachment = c;
    }
    if (depth_stencil) |d| {
        rt.mask.depth_stencil = true;
        rt.depth_stencil_attachment = d;
    }
    if (!rt.mask.color and !rt.mask.depth_stencil) return null_render_target;
    if (!rt.mask.color or !rt.mask.depth_stencil) {
        const t: *VKTexture = .get(if (color) |c| c else depth_stencil.?);
        _ = try VKTexture.recreateIfNeeded(@ptrCast(t), &.{t.capabilities.add(.{ .attachment = true })});
        return try rt.append();
    }

    const ct, const dst = .{ VKTexture.get(color.?), VKTexture.get(depth_stencil.?) };

    // if (!std.meta.eql(ct.params, dst.params))
    //     return Error.NonMatchingRenderAttachmentParams;
    var arr: [2]VKTexture = .{ ct.*, dst.* };
    const target_capabilites = ct.capabilities.add(dst.capabilities).add(.{ .attachment = true });
    if (try VKTexture.recreateIfNeeded(&arr, &.{target_capabilites})) {
        ct.* = arr[0];
        dst.* = arr[1];
    }
    return try rt.append();
}
// renderTargetFromTextures: *const RendrerTargetFromTexturesFn = undefined,
fn createRenderTarget(
    size: math.uvec2,
    color_format: ?gpu.Format,
    depth_stencil_format: ?gpu.Format,
    params: gpu.TextureParams,
) Error!RenderTarget {
    if (color_format == null and depth_stencil_format == null) return null_render_target;
    const render_target = try VKRenderTarget.create(size, color_format, depth_stencil_format, params);
    return try render_target.append();
}
fn destroyRenderTarget(handle: RenderTarget) void {
    VKRenderTarget.get(handle).destroy();
}

// createRenderTarget: *const CreateRenderTargetFn = undefined,
// destroyRenderTarget: *const DestroyRenderTargetFn = undefined,

fn getTextureType(handle: Texture) gpu.TextureType {
    const t = VKTexture.get(handle);

    if (t.size[2] > 1) return .@"3d";
    if (t.size[1] <= 1) return if (t.params.array_layers > 1) .@"1d_array" else .@"1d";

    return if (t.params.cubemap) ( //
        return if (t.params.array_layers > 1) .cube_array else .cube //
    ) else ( //
        return if (t.params.array_layers > 1) .@"2d_array" else .@"2d" //
    );
}
fn getTextureSize(handle: Texture) math.uvec3 {
    return VKTexture.get(handle).size;
}
fn getTextureFormat(handle: Texture) Format {
    return VKTexture.get(handle).format;
}
fn createTexture(size: math.uvec3, format: Format, params: gpu.TextureParams) Error!Texture {
    const texture: VKTexture = try .create(size, format, params, .{
        .transfer = true,
    });
    return try texture.append();
}
fn destroyTexture(handle: Texture) void {
    VKTexture.get(handle).destroy();
}
fn createBuffer(size: usize, usage: BufferUsage) Error!Buffer {
    const buffer = try VKBuffer.create(@intCast(size), usage);
    const handle: Buffer = @enumFromInt(@as(gpu.Handle, @intCast(buffer_list.items.len)));
    try buffer_list.append(arena.allocator(), buffer);
    return handle;
}
fn destroyBuffer(handle: Buffer) void {
    VKBuffer.get(handle).destroy();
}

fn renderTargetSize(render_target: RenderTarget) math.uvec2 {
    if (render_target == null_render_target) return @splat(0);

    return if (VKWindowContext.fromRenderTarget(render_target)) |wc|
        .{ wc.extent.width, wc.extent.height }
    else
        VKRenderTarget.get(render_target).size();
    // math.swizzle(VKTexture.fromRenderTarget(render_target).size, .xy);
}

fn updateWindowContext(handle: WindowContext) void {
    _ = handle;
}

fn getWindowRenderTarget(window: huge.Window) RenderTarget {
    return getWindowContextRenderTarget(window.context);
}
fn getWindowContextRenderTarget(handle: WindowContext) RenderTarget {
    return @enumFromInt((1 << 31) | @intFromEnum(handle));
}
fn createWindowContext(window: huge.Window) Error!WindowContext {
    const wc = VKWindowContext.create(window) catch
        return Error.WindowContextCreationError;
    if (window_context_count == 0) {
        window_context_primary = wc;
        return @enumFromInt(0);
    } else {
        @panic("VK multiple window contexts");
    }
}
fn destroyWindowContext(handle: WindowContext) void {
    const window_context = VKWindowContext.get(handle);
    window_context.destroy(handle);
}

//===|implementations|===
const VKShaderModule = struct {
    const LoadState = enum { unloaded, fail, success };
    load_state: LoadState = .unloaded,
    vk_handle: vk.ShaderModule = .null_handle,

    entry_point_info: hgsl.EntryPointInfo = .{},
    path: []const u8 = "",
    entry_point: []const u8 = "",

    pub fn recreate(self: *VKShaderModule) void {
        if (self.load_state == .unloaded) {
            const new = createPath(self.path, self.entry_point) catch {
                self.load_state = .fail;
                return;
            };
            device.destroyShaderModule(self.vk_handle, vka);
            self.* = new;
        }
    }

    pub fn createPath(path: []const u8, entry_point: []const u8) Error!VKShaderModule {
        const result = shader_compiler.compileFile(path) catch
            return Error.ShaderCompilationError;
        const vk_handle: vk.ShaderModule = device.createShaderModule(&.{
            .code_size = result.bytes.len,
            .p_code = @ptrCast(@alignCast(result.bytes.ptr)),
        }, vka) catch
            return Error.OutOfMemory;

        return .{
            .load_state = .success,
            .path = path,
            .entry_point = entry_point,
            .vk_handle = vk_handle,
            .entry_point_info = for (result.entry_point_infos) |ep| {
                if (util.strEql(entry_point, ep.name)) break ep;
            } else return Error.ShaderEntryPointNotFound,
        };
        // const result = shader_compiler.compileFile(path);
    }
    pub fn createSource(source: []const u8, entry_point: []const u8) Error!VKShaderModule {
        if (true) @panic("VKShaderModule.createRaw");
        return try createPath(source, entry_point);
    }
    pub fn destroy(self: *VKShaderModule) void {
        device.destroyShaderModule(self.vk_handle, vka);
        self.* = .{};
    }

    pub fn get(handle: ShaderModule) *VKShaderModule {
        return &shader_module_list.items[@intFromEnum(handle)];
    }
};
//handle array with functions that have explicit
//(offset and size) or (binding) args on top of 'name'
const VKPipeline = struct {
    vk_handle: vk.Pipeline = .null_handle,
    layout: vk.PipelineLayout = .null_handle,

    stage_count: u32 = 0,
    shader_modules: [Pipeline.max_pipeline_stages]ShaderModule = undefined,
    entry_point_info_storage: [Pipeline.max_pipeline_stages]hgsl.EntryPointInfo = undefined,
    push_constant_ranges: [Pipeline.max_pipeline_stages]vk.PushConstantRange = undefined,
    push_constant_range_count: u32 = 0,
    params: gpu.PipelineParams = .{},

    pub fn recreate(self: *VKPipeline) Error!void {
        const sms = self.shader_modules[0..self.stage_count];
        for (sms) |sm| VKShaderModule.get(sm).recreate();
        if (!for (sms) |sm| {
            if (VKShaderModule.get(sm).load_state == .fail)
                break false;
        } else true) return;
        const new = create(sms, self.params) catch return;
        device.destroyPipelineLayout(self.layout, vka);
        device.destroyPipeline(self.vk_handle, vka);
        self.* = new;
    }
    //descriptor_set
    pub fn create(stage_modules: []const ShaderModule, params: gpu.PipelineParams) Error!VKPipeline {
        var pipeline: VKPipeline = .{ .params = params };
        @memcpy(pipeline.shader_modules[0..stage_modules.len], stage_modules);

        const mps = gpu.Pipeline.max_pipeline_stages;
        var stage_create_infos: [mps]vk.PipelineShaderStageCreateInfo = undefined;

        for (0..stage_modules.len) |i| {
            const shader_module: *VKShaderModule = .get(stage_modules[i]);
            const ep_info = shader_module.entry_point_info;
            if (i != 0) {
                const previous_stage_ep_info = VKShaderModule.get(stage_modules[i - 1]).entry_point_info;
                for (ep_info.inputMappings()) |im| {
                    const @"type" = for (previous_stage_ep_info.outputMappings()) |om| {
                        if (om.location == im.location) break om.type;
                    } else return Error.PipelineStageIOMismatch;
                    if (!im.type.eql(@"type")) return Error.PipelineStageIOMismatch;
                }
            }
            const stage_flags = getStageFlags(ep_info.stage_info);
            stage_create_infos[i] = .{
                .module = shader_module.vk_handle,
                .p_name = ep_info.name,
                .stage = stage_flags,
            };
            pipeline.entry_point_info_storage[i] = ep_info;
            pipeline.stage_count = @intCast(stage_modules.len);

            if (ep_info.push_constant_mappings.len != 0) {
                const last_pc = ep_info.push_constant_mappings[ep_info.push_constant_mappings.len - 1];
                pipeline.push_constant_ranges[pipeline.push_constant_range_count] = .{
                    .stage_flags = stage_flags,
                    .offset = 0,
                    .size = last_pc.offset + last_pc.size,
                };
                pipeline.push_constant_range_count += 1;
            }
        }
        pipeline.sortRanges();
        //CHECK stages *compatibility

        const fragment: ?FragmentStageInfo = FragmentStageInfo.fromStageSlice(stage_modules);
        const vertex: ?VertexStageInfo = VertexStageInfo.fromStageSlice(stage_modules);

        const layout = device.createPipelineLayout(&.{
            // set_layout_count: u32 = 0,
            // p_set_layouts: ?[*]const DescriptorSetLayout = null,
            .push_constant_range_count = pipeline.push_constant_range_count,
            .p_push_constant_ranges = &pipeline.push_constant_ranges,
        }, vka) catch return Error.ResourceCreationError;
        pipeline.layout = layout;

        _ = device.createGraphicsPipelines(.null_handle, 1, &.{.{
            .p_next = if (fragment) |frag| &(try frag.pipelineCreationInfo(
                &.{.b8g8r8a8_unorm},
                .undefined,
                .undefined,
            )) else null,
            .stage_count = @intCast(stage_modules.len),
            .p_stages = stage_create_infos[0..stage_modules.len].ptr,
            .layout = layout,

            .p_vertex_input_state = if (vertex) |vert| &.{
                .vertex_binding_description_count = 1,
                .p_vertex_binding_descriptions = &.{vert.binding_description},
                .vertex_attribute_description_count = @intCast(vert.attributes.len),
                .p_vertex_attribute_descriptions = vert.attributes.ptr,
            } else null,
            .p_input_assembly_state = if (vertex) |_| &.{
                .topology = castPrimitiveTopology(params.primitive),
                .primitive_restart_enable = .false,
            } else null,
            // p_tessellation_state: ?*const PipelineTessellationStateCreateInfo = null,
            .p_viewport_state = if (fragment) |frag| &.{
                .viewport_count = frag.output_count,
                .scissor_count = frag.output_count,
            } else null,
            // p_rasterization_state: ?*const PipelineRasterizationStateCreateInfo = null,
            .p_rasterization_state = &.{
                .depth_clamp_enable = .false,
                .rasterizer_discard_enable = .false,
                .polygon_mode = .fill,
                .line_width = 1,
                .cull_mode = .{
                    .back_bit = params.cull == .back or params.cull == .both,
                    .front_bit = params.cull == .front or params.cull == .both,
                },
                .front_face = if (params.winding_order == .clockwise) .clockwise else .counter_clockwise,
                .depth_bias_enable = .false,
                .depth_bias_constant_factor = 0,
                .depth_bias_clamp = 0,
                .depth_bias_slope_factor = 0,
            },

            .p_multisample_state = &.{
                .sample_shading_enable = .false,
                .rasterization_samples = .{ .@"1_bit" = true },
                .min_sample_shading = 1,
                .p_sample_mask = null,
                .alpha_to_coverage_enable = .false,
                .alpha_to_one_enable = .false,
            },
            // p_depth_stencil_state: ?*const PipelineDepthStencilStateCreateInfo = null,
            .p_color_blend_state = if (fragment) |frag| &.{
                .logic_op_enable = .false,
                .logic_op = .clear,
                .attachment_count = frag.output_count,
                .p_attachments = &.{.{
                    .color_write_mask = .{
                        .r_bit = true,
                        .b_bit = true,
                        .g_bit = true,
                        .a_bit = true,
                    },
                    .blend_enable = .false,
                    .src_color_blend_factor = .one,
                    .dst_color_blend_factor = .zero,
                    .color_blend_op = .add,
                    .src_alpha_blend_factor = .one,
                    .dst_alpha_blend_factor = .zero,
                    .alpha_blend_op = .add,
                }},
                .blend_constants = @splat(0),
            } else null,
            .p_dynamic_state = &.{
                .p_dynamic_states = &.{ .viewport, .scissor },
                .dynamic_state_count = 2,
            },

            .subpass = 0,
            .base_pipeline_index = -1,
            .flags = .{},
        }}, vka, @ptrCast(&pipeline.vk_handle)) catch
            return Error.ResourceCreationError;
        return pipeline;
    }
    pub fn destroy(self: *VKPipeline) void {
        device.destroyPipelineLayout(self.layout, vka);
        device.destroyPipeline(self.vk_handle, vka);
        self.* = .{};
    }
    fn sortRanges(self: *VKPipeline) void {
        if (self.push_constant_range_count < 2) return;
        for (0..self.push_constant_range_count - 1) |i| {
            for (i + 1..self.push_constant_range_count) |j| {
                if (self.push_constant_ranges[i].size < self.push_constant_ranges[j].size)
                    std.mem.swap(vk.PushConstantRange, &self.push_constant_ranges[i], &self.push_constant_ranges[j]);
            }
        }
    }
    pub fn get(handle: Pipeline) *VKPipeline {
        return &pipeline_list.items[@intFromEnum(handle)];
    }
};
const FragmentStageInfo = struct {
    output_count: u32,

    pub fn fromStageSlice(stages: []const ShaderModule) ?FragmentStageInfo {
        const ep_info: hgsl.EntryPointInfo = for (stages) |s| {
            const shader_module: *VKShaderModule = VKShaderModule.get(s);
            if (shader_module.entry_point_info.stage_info == .fragment)
                break shader_module.entry_point_info;
        } else return null;
        _ = ep_info;
        return .{
            .output_count = 1, //TODO: check that
        };
    }
    pub fn pipelineCreationInfo(
        self: *const FragmentStageInfo,
        formats: []const vk.Format,
        depth_format: vk.Format,
        stencil_format: vk.Format,
    ) Error!vk.PipelineRenderingCreateInfo {
        //
        if (formats.len != self.output_count) return Error.ResourceCreationError;
        return .{
            .color_attachment_count = self.output_count,
            .p_color_attachment_formats = formats.ptr,
            .depth_attachment_format = depth_format,
            .stencil_attachment_format = stencil_format,
            .view_mask = 0,
        };
    }
};
const VertexStageInfo = struct {
    attributes: []const vk.VertexInputAttributeDescription = &.{},
    binding_description: vk.VertexInputBindingDescription,
    var attribute_storage: [10]vk.VertexInputAttributeDescription = undefined;

    pub fn fromStageSlice(stages: []const ShaderModule) ?VertexStageInfo {
        const ep_info: hgsl.EntryPointInfo = for (stages) |s| {
            const shader_module: *VKShaderModule = .get(s);
            if (shader_module.entry_point_info.stage_info == .vertex)
                break shader_module.entry_point_info;
        } else return null;
        if (ep_info.inputMappings().len > attribute_storage.len)
            @panic("TODO: unlimited vertex attributes");

        var stride: u32 = 0;
        for (ep_info.inputMappings(), 0..) |im, i| {
            attribute_storage[i] = .{
                .location = im.location,
                .binding = 0,
                .format = formatFromIOType(im.type),
                .offset = stride,
            };
            stride += im.size;
        }
        return .{
            .attributes = attribute_storage[0..ep_info.input_count],
            .binding_description = .{
                .binding = 0,
                .stride = stride,
                .input_rate = .vertex,
            },
        };
    }
};
const VKTexture = struct {
    image: vk.Image = .null_handle,
    view: vk.ImageView = .null_handle,
    sampler: vk.Sampler = .null_handle,
    memory: u32 = u32m,

    size: math.uvec3 = @splat(1),
    format: Format = .rgba8_norm,
    params: gpu.TextureParams = .{},

    capabilities: Capabilites = .{},
    vk_format: vk.Format = .undefined,

    const max_textures_per_allocation = 16;
    const mt = max_textures_per_allocation;

    pub fn recreateIfNeeded(
        textures: []VKTexture,
        capabilities: []const Capabilites,
    ) Error!bool {
        if (textures.len == 0) return false;
        var recreated = false;
        if (textures.len > mt)
            recreated = try recreateIfNeeded(
                textures[mt..],
                if (capabilities.len > 0) capabilities[@min(mt, capabilities.len - 1)..] else &.{},
            );

        var capability_storage: [mt]Capabilites = @splat(.{});
        if (capabilities.len > 0) for (0..mt) |i| {
            capability_storage[i] = capabilities[@min(i, capabilities.len - 1)];
        };

        const tex = textures[0..@min(textures.len, mt)];

        if (for (tex, 0..) |t, i| {
            if (!capability_storage[i].isSubset(t.capabilities)) break true;
        } else false) {
            var params_storage: [mt]TextureCreateParams = undefined;
            for (tex, 0..) |*t, i| {
                t.destroyHandles();
                params_storage[i] = .{
                    .size = t.size,
                    .format = t.format,
                    .capabilities = capability_storage[i],
                };
            }
            try createSlice(tex, params_storage[0..tex.len]);
            return true;
        }
        return recreated;
    }
    pub fn create(
        size: math.uvec3,
        format: Format,
        params: gpu.TextureParams,
        capabilities: VKTexture.Capabilites,
    ) Error!VKTexture {
        var t: VKTexture = undefined;
        try createSlice(@ptrCast(&t), &.{.{
            .size = size,
            .format = format,
            .params = params,
            .capabilities = capabilities,
        }});
        return t;
    }

    pub fn createSlice(
        output: []VKTexture,
        params: []const TextureCreateParams,
    ) Error!void {
        if (output.len == 0) return;
        if (output.len > mt)
            try createSlice(
                output[mt..],
                if (params.len > 0) params[@min(mt, params.len - 1)..] else &.{},
            );

        var param_storage: [mt]TextureCreateParams = @splat(.{});
        if (params.len > 0) for (0..mt) |i| {
            param_storage[i] = params[@min(i, params.len - 1)];
        };
        const tex = output[0..@min(output.len, mt)];
        for (tex, 0..) |*t, i| {
            const p = param_storage[i];
            var new_size = @max(p.size, @as(math.uvec3, @splat(1)));
            if (new_size[1] == 1) new_size[2] = 1;

            t.* = .{
                .format = p.format,
                .size = new_size,
                .params = p.params,
                .capabilities = p.capabilities,
            };
            t.params.array_layers = @max(1, t.params.array_layers);
            const image_type: vk.ImageType = if (t.size[1] == 1)
                .@"1d"
            else if (t.size[2] == 1) .@"2d" else .@"3d";
            if ((image_type != .@"2d" and t.params.cubemap) or
                (image_type == .@"3d" and t.params.array_layers > 1))
                return Error.InvalidImageType;

            const format_usage: FormatUsage = .{
                .sampled = t.params.filtering != null,
                .sampled_linear = if (t.params.filtering) |f|
                    f.shrink == .linear or f.expand == .linear
                else
                    false,

                .color_attachment = !p.format.isDepthStencil() and p.capabilities.attachment,
                .depth_stencil_attachment = p.format.isDepthStencil() and p.capabilities.attachment,

                .blit_src = p.capabilities.blit,
                .blit_dst = p.capabilities.blit,
                .transfer_src = p.capabilities.transfer,
                .transfer_dst = p.capabilities.transfer,
            };

            //if no usage dont create
            if (@as(@typeInfo(FormatUsage).@"struct".backing_integer.?, @bitCast(format_usage)) == 0)
                continue;
            var linear_tiling = false;
            t.vk_format = getVulkanFormat(p.format, format_usage, .image_optimal);
            std.debug.print("VKFORMAT:{} => {}\n", .{ t.format, t.vk_format });
            if (t.vk_format == .undefined) {
                linear_tiling = true;
                t.vk_format = getVulkanFormat(p.format, format_usage, .image_linear);
            }

            const view_mutable_format = false;
            t.image = device.createImage(&.{
                .image_type = image_type,
                .flags = .{
                    .mutable_format_bit = view_mutable_format,
                    .cube_compatible_bit = t.params.cubemap,
                },
                .format = t.vk_format,
                .extent = .{
                    .width = t.size[0],
                    .height = t.size[1],
                    .depth = t.size[2],
                },
                .mip_levels = t.params.mip_levels,
                .array_layers = t.params.array_layers,
                .samples = .{ .@"1_bit" = true },
                .tiling = if (linear_tiling) .linear else .optimal,
                .usage = format_usage.toImageUsage(),
                .initial_layout = .undefined,
                .sharing_mode = .exclusive,
                // sharing_mode: SharingMode,
                // queue_family_index_count: u32 = 0,
                // p_queue_family_indices: ?[*]const u32 = null,
            }, vka) catch return Error.ResourceCreationError;
        }
        var memory_allocations: [mt]struct {
            req: vk.MemoryRequirements,
            allocation: u32,
        } = undefined;
        var index_map: [mt]struct {
            index: usize,
            offset: u64 = 0,
        } = undefined;

        var mem_req_index: usize = 0;
        for (tex, 0..) |*t, i| {
            if (t.image == .null_handle) continue;
            const req = device.getImageMemoryRequirements(t.image);
            for (memory_allocations[0..mem_req_index], 0..) |*ma, j| {
                if (req.memory_type_bits == ma.req.memory_type_bits and req.alignment == ma.req.alignment) {
                    const offset = util.rut(u64, ma.req.size, ma.req.alignment);
                    ma.req.size += req.size + (offset - ma.req.size);
                    index_map[i] = .{ .index = j, .offset = offset };
                    break;
                }
            } else {
                memory_allocations[mem_req_index].req = req;
                index_map[i] = .{ .index = mem_req_index };
                mem_req_index += 1;
            }
        }
        for (memory_allocations[0..mem_req_index]) |*ma|
            ma.allocation = try allocateDeviceMemory(ma.req, .{});

        for (tex, 0..) |*t, i| {
            if (t.image == .null_handle) continue;
            t.memory = memory_allocations[index_map[i].index].allocation;
            device.bindImageMemory(t.image, getMemoryReference(t.memory), index_map[i].offset) catch
                return Error.ResourceCreationError;
            const mask = frmt.mask(t.vk_format);
            t.view = device.createImageView(&.{
                .image = t.image,
                .view_type = switch (@as(vk.ImageType, if (t.size[1] == 1)
                    .@"1d"
                else if (t.size[2] == 1) .@"2d" else .@"3d")) {
                    .@"1d" => if (t.params.array_layers > 1) .@"1d_array" else .@"1d",
                    .@"2d" => if (t.params.cubemap)
                        (if (t.params.array_layers > 1) .cube_array else .cube)
                    else
                        (if (t.params.array_layers > 1) .@"2d_array" else .@"2d"),
                    .@"3d" => .@"3d",
                    _ => unreachable,
                },
                .format = t.vk_format,
                .components = .{ .r = .identity, .g = .identity, .b = .identity, .a = .identity },
                .subresource_range = .{
                    .aspect_mask = .{
                        .color_bit = mask.color,
                        .depth_bit = mask.depth,
                        .stencil_bit = mask.stencil,
                    },
                    .layer_count = t.params.array_layers,
                    .base_array_layer = 0,
                    .level_count = t.params.mip_levels,
                    .base_mip_level = 0,
                },
            }, vka) catch
                return Error.ResourceCreationError;
        }
    }
    pub fn destroy(self: *VKTexture) void {
        self.destroyHandles();
        self.* = .{};
    }
    fn destroyHandles(self: *VKTexture) void {
        device.destroyImage(self.image, vka);
        self.image = .null_handle;
        device.destroyImageView(self.view, vka);
        self.view = .null_handle;
        device.destroySampler(self.sampler, vka);
        self.sampler = .null_handle;
        if (~self.memory != 0)
            removeMemoryReference(self.memory);
        self.memory = ~@as(u32, 0);
    }
    pub fn append(self: VKTexture) Error!Texture {
        const handle: Texture = @enumFromInt(@as(gpu.Handle, @intCast(texture_list.items.len)));
        try texture_list.append(arena.allocator(), self);
        return handle;
    }
    pub fn get(handle: Texture) *VKTexture {
        return &texture_list.items[@intFromEnum(handle)];
    }
    pub const Capabilites = packed struct(u3) {
        attachment: bool = false,
        blit: bool = false,
        transfer: bool = false,

        pub fn add(self: Capabilites, other: Capabilites) Capabilites {
            const U = @typeInfo(@This()).@"struct".backing_integer.?;
            return @bitCast(@as(U, @bitCast(self)) | @as(U, @bitCast(other)));
        }
        pub fn isSubset(self: Capabilites, superset: Capabilites) bool {
            const U = @typeInfo(@This()).@"struct".backing_integer.?;
            return @as(U, @bitCast(self)) == (@as(U, @bitCast(self)) & @as(U, @bitCast(superset)));
        }
    };
};
pub const TextureCreateParams = struct {
    size: math.uvec3 = @splat(1),
    format: Format = .rgba8_norm,
    params: gpu.TextureParams = .{},
    capabilities: VKTexture.Capabilites = .{},
};

const VKBuffer = struct {
    vk_handle: vk.Buffer = .null_handle,
    device_memory: u32 = u32m,
    size: u64 = 0,
    usage: BufferUsage = undefined,

    mapped: bool = false,
    pub fn create(size: u64, usage: BufferUsage) Error!VKBuffer {
        if (size == 0) return .{ .size = 0, .usage = usage };

        const handle = device.createBuffer(&.{
            .size = size,
            .usage = switch (usage) {
                .vertex => .{ .vertex_buffer_bit = true },
                .index => .{ .index_buffer_bit = true },
                .uniform => .{ .uniform_buffer_bit = true },
                else => .{},
            },
            .sharing_mode = .exclusive,
            // .p_queue_family_indices = &.{},
            // .queue_family_index_count = 1,
        }, vka) catch return Error.ResourceCreationError;

        const memory_requirements = device.getBufferMemoryRequirements(handle);
        const memory = try allocateDeviceMemory(memory_requirements, .{ .host_visible_bit = true });

        device.bindBufferMemory(handle, getMemoryReference(memory), 0) catch
            return Error.ResourceCreationError;
        return .{
            .vk_handle = handle,
            .device_memory = memory,
            .size = size,
            .usage = usage,
        };
    }
    pub fn destroy(self: *VKBuffer) void {
        device.destroyBuffer(self.vk_handle, vka);
        removeMemoryReference(self.device_memory);
        self.* = .{};
    }
    pub fn get(handle: Buffer) *VKBuffer {
        return &buffer_list.items[@intFromEnum(handle)];
    }
};

fn accessMemory(index: u32) vk.DeviceMemory {
    return memory_allocation_list.items[index].memory;
}
fn getMemoryReference(index: u32) vk.DeviceMemory {
    const ptr = &memory_allocation_list.items[index];
    ptr.ref_count += 1;
    return ptr.memory;
}
fn removeMemoryReference(index: u32) void {
    const ptr = &memory_allocation_list.items[index];
    ptr.ref_count -|= 1;
    if (ptr.ref_count == 0) ptr.free();
}
fn allocateDeviceMemory(memory_requirements: vk.MemoryRequirements, flags: vk.MemoryPropertyFlags) Error!u32 {
    std.debug.print("ALLOC: {d} KiB\n", .{@as(f64, @floatFromInt(memory_requirements.size)) / 1024.0});
    const mem_type: u32 = for (0..physical_device_memory_properties.memory_type_count) |i| {
        if ((memory_requirements.memory_type_bits & (@as(u32, 1) << @as(u5, @intCast(i)))) == 0) continue;
        if (physical_device_memory_properties.memory_types[i].property_flags.contains(flags))
            break @intCast(i);
    } else return Error.ResourceCreationError;

    const device_memory = device.allocateMemory(&.{
        .allocation_size = memory_requirements.size,
        .memory_type_index = mem_type,
    }, vka) catch
        return Error.ResourceCreationError;
    const index: u32 = @intCast(memory_allocation_list.items.len);
    try memory_allocation_list.append(arena.allocator(), .{ .memory = device_memory });
    return index;
}

const VKRenderTarget = struct {
    color_attachment: Texture = @enumFromInt(0),
    depth_stencil_attachment: Texture = @enumFromInt(0),
    mask: Mask = .{},

    pub fn size(self: VKRenderTarget) math.uvec2 {
        const ct: *VKTexture = if (self.mask.color)
            .get(self.color_attachment)
        else
            .get(self.depth_stencil_attachment);
        return .{ ct.size[0], ct.size[1] };
    }

    pub fn create(
        tex_size: math.uvec2,
        color_format: ?Format,
        depth_stencil_format: ?Format,
        params: gpu.TextureParams,
    ) Error!VKRenderTarget {
        if (color_format == null and depth_stencil_format == null)
            @panic("both formats are null(should be checked before calling)");
        var result: VKRenderTarget = .{ .mask = .{
            .color = color_format != null,
            .depth_stencil = depth_stencil_format != null,
        } };
        var tex_create_params: [2]TextureCreateParams = @splat(.{
            .size = math.swizzle(tex_size, .xy0),
            .params = params,
            .capabilities = .{ .attachment = true },
        });
        var i: usize = 0;
        if (color_format) |f| tex_create_params[util.ipp(&i)].format = f;
        if (depth_stencil_format) |f| tex_create_params[util.ipp(&i)].format = f;
        var textures: [2]VKTexture = undefined;
        try VKTexture.createSlice(&textures, tex_create_params[0..i]);

        if (color_format) |_| result.color_attachment = try textures[0].append();
        if (depth_stencil_format) |_| result.depth_stencil_attachment = try textures[i - 1].append();
        return result;
    }
    pub fn destroy(self: *VKRenderTarget) void {
        self.mask = .{};
    }
    pub fn append(self: VKRenderTarget) Error!RenderTarget {
        const handle: RenderTarget = @enumFromInt(@as(gpu.Handle, @intCast(render_target_list.items.len)));
        try render_target_list.append(arena.allocator(), self);
        return handle;
    }
    pub fn get(handle: RenderTarget) *VKRenderTarget {
        return &render_target_list.items[~@as(gpu.Handle, 1 << 31) & @intFromEnum(handle)];
    }

    const Mask = packed struct { color: bool = false, depth_stencil: bool = false };
};
fn getRenderingResources() Error!vk.CommandBuffer {
    huge.dassert(current_render_target != null_render_target);
    if (current_render_target == null_render_target) return .null_handle;
    return if (VKWindowContext.fromRenderTarget(current_render_target)) |wc|
        try wc.initRenderingCmd()
    else
        @panic("init rendering resources NOT of WINDOW rt");
}

const VKWindowContext = struct {
    const mic = 3; //max_image_count
    const mfif = mic - 1; //max_frame_in_flight
    acquired_image_index: u32 = u32m,

    fif_index: u32 = 0, //current frame-in-flight index

    surface: vk.SurfaceKHR = .null_handle,
    request_recreate: bool = false,
    swapchain: vk.SwapchainKHR = .null_handle,

    images: [mic]vk.Image = @splat(.null_handle),
    image_views: [mic]vk.ImageView = @splat(.null_handle),
    image_count: u32 = 0,

    extent: vk.Extent2D = .{ .width = 0, .height = 0 },
    surface_format: vk.SurfaceFormatKHR = undefined,
    present_mode: vk.PresentModeKHR = .fifo_khr,

    current_frame: usize = 0,
    acquire_semaphores: [mfif]vk.Semaphore = @splat(.null_handle),
    submit_semaphores: [mic]vk.Semaphore = @splat(.null_handle),
    fences: [mfif]vk.Fence = @splat(.null_handle),

    //render target
    rendering_cmds: [mfif * queue_type_count]vk.CommandBuffer = @splat(.null_handle),
    clear_value: ClearValue = undefined,

    inline fn fif(self: VKWindowContext) u32 {
        return @max(@max(self.image_count, 1) - 1, 1);
    }
    pub fn setRenderTargetState(self: *VKWindowContext, clear_value: ClearValue) Error!void {
        self.clear_value = clear_value;
        self.fif_index = (self.fif_index + 1) % self.fif();
    }
    pub fn initRenderingCmd(self: *VKWindowContext) Error!vk.CommandBuffer {
        if (current_render_target == null_render_target) return Error.Unknown;

        const cmd = try self.getRenderingCmd(.graphics);
        if (~self.acquired_image_index != 0) return cmd;

        _ = device.waitForFences(1, &.{self.fences[self.fif_index]}, .true, timeout) catch
            return Error.SynchronisationError;
        device.resetFences(1, &.{self.fences[self.fif_index]}) catch
            return Error.SynchronisationError;
        self.acquired_image_index = (device.acquireNextImageKHR(
            self.swapchain,
            timeout,
            self.acquire_semaphores[self.fif_index],
            .null_handle,
        ) catch return Error.ResourceCreationError).image_index;

        device.resetCommandBuffer(cmd, .{}) catch
            return Error.ResourceCreationError;

        device.beginCommandBuffer(cmd, &.{}) catch return Error.Unknown;
        const rt_size = renderTargetSize(current_render_target);
        device.cmdSetScissor(cmd, 0, 1, &.{.{
            .extent = .{ .width = rt_size[0], .height = rt_size[1] },
            .offset = .{ .x = 0, .y = 0 },
        }});
        device.cmdSetViewport(cmd, 0, 1, &.{.{
            .x = 0,
            .y = 0,
            .width = @floatFromInt(rt_size[0]),
            .height = @floatFromInt(rt_size[1]),
            .min_depth = 0,
            .max_depth = 1,
        }});
        device.cmdBeginRendering(cmd, &.{
            .render_area = .{
                .offset = .{ .x = 0, .y = 0 },
                .extent = self.extent,
            },
            .color_attachment_count = 1,
            .p_color_attachments = &.{.{
                .image_view = self.image_views[self.acquired_image_index],
                .image_layout = .attachment_optimal,
                .load_op = .clear,
                .store_op = .store,
                .clear_value = .{
                    .color = if (self.clear_value.color) |cc|
                        .{ .float_32 = @as(*const [4]f32, @ptrCast(&cc)).* }
                    else
                        .{ .float_32 = @splat(0) },
                },

                .resolve_image_layout = .undefined,
                .resolve_mode = .{},
            }},

            .flags = .{},
            .layer_count = 1,
            .view_mask = 0,
            // p_depth_attachment: ?*const RenderingAttachmentInfo = null,
            // p_stencil_attachment: ?*const RenderingAttachmentInfo = null,
        });
        return cmd;
    }

    pub fn endRendering(self: *VKWindowContext) Error!void {
        defer self.acquired_image_index = u32m;

        const cmd = self.getRenderingCmdOpt(.graphics);
        if (cmd == .null_handle) return;
        device.cmdEndRendering(cmd);
        self.present(cmd) catch
            return Error.PresentationError;
    }
    pub fn forceEndRendering(self: *VKWindowContext) void {
        defer self.acquired_image_index = u32m;

        const cmd = self.getRenderingCmdOpt(.graphics);
        if (cmd == .null_handle) return;
        device.cmdEndRendering(cmd);
        device.endCommandBuffer(cmd) catch {};
        device.queueSubmit(queue(.presentation), 1, &.{.{
            .command_buffer_count = 0,
            .p_wait_dst_stage_mask = &.{.{ .color_attachment_output_bit = true }},
            .wait_semaphore_count = 1,
            .p_wait_semaphores = &.{self.acquire_semaphores[self.fif_index]},
        }}, self.fences[self.fif_index]) catch {};
    }
    fn getRenderingCmd(self: *VKWindowContext, queue_type: QueueType) Error!vk.CommandBuffer {
        const ptr = &self.rendering_cmds[@intFromEnum(queue_type) + self.fif_index * queue_type_count];
        if (ptr.* == .null_handle)
            ptr.* = try allocCommandBuffer(queue_type, .primary);
        return ptr.*;
    }
    fn getRenderingCmdOpt(self: *VKWindowContext, queue_type: QueueType) vk.CommandBuffer {
        return self.rendering_cmds[@intFromEnum(queue_type) + self.fif_index * queue_type_count];
    }

    fn present(self: VKWindowContext, cmd: vk.CommandBuffer) !void {
        const image_barrier: vk.ImageMemoryBarrier = .{
            .src_access_mask = .{ .color_attachment_write_bit = true },
            .dst_access_mask = .{},
            .old_layout = .undefined,
            .new_layout = .present_src_khr,
            .src_queue_family_index = pd().queueFamilyIndex(.presentation),
            .dst_queue_family_index = pd().queueFamilyIndex(.presentation),
            .image = self.images[self.acquired_image_index],
            .subresource_range = .{
                .aspect_mask = .{ .color_bit = true },
                .base_mip_level = 0,
                .level_count = 1,
                .base_array_layer = 0,
                .layer_count = 1,
            },
        };
        device.cmdPipelineBarrier(cmd, .{
            .all_commands_bit = true,
        }, .{
            .bottom_of_pipe_bit = true,
        }, .{}, 0, null, 0, null, 1, &.{
            image_barrier,
        });

        try device.endCommandBuffer(cmd);
        try device.queueSubmit(
            queue(.presentation),
            1,
            &.{.{
                .command_buffer_count = 1,
                .p_command_buffers = &.{cmd},
                .p_wait_dst_stage_mask = &.{.{ .color_attachment_output_bit = true }},

                .wait_semaphore_count = 1,
                .p_wait_semaphores = &.{self.acquire_semaphores[self.fif_index]},
                .signal_semaphore_count = 1,
                .p_signal_semaphores = &.{self.submit_semaphores[self.acquired_image_index]},
            }},
            self.fences[self.fif_index],
        );

        _ = device.queuePresentKHR(queue(.presentation), &.{
            .wait_semaphore_count = 1,
            .p_wait_semaphores = &.{self.submit_semaphores[self.acquired_image_index]},
            .swapchain_count = 1,
            .p_swapchains = &.{self.swapchain},
            .p_image_indices = &.{self.acquired_image_index},
        }) catch |err|
            switch (err) {
                error.OutOfDateKHR => {
                    @panic("recreate swapchain");
                    // self.request_recreate = true;
                },
                else => return error.PresentationError,
            };
    }
    fn fromRenderTarget(render_target: RenderTarget) ?*VKWindowContext {
        const int: gpu.Handle = @intFromEnum(render_target);
        if ((int >> 31) == 0) return null;
        return VKWindowContext.get(@enumFromInt(int & ~@as(gpu.Handle, 1 << 31)));
    }

    pub fn create(window: huge.Window) !VKWindowContext {
        var surface_handle: u64 = undefined;
        if (glfw.createWindowSurface(
            @intFromEnum(instance.handle),
            window.handle,
            null,
            &surface_handle,
        ) != .success) return Error.WindowContextCreationError;
        var result: VKWindowContext = .{
            .surface = @enumFromInt(surface_handle),
        };

        const capabilities = try instance.getPhysicalDeviceSurfaceCapabilitiesKHR(pd().handle, result.surface);

        result.extent = blk: {
            if (capabilities.current_extent.width == std.math.maxInt(u32)) {
                var res: [2]c_int = @splat(0);
                glfw.getFramebufferSize(window.handle, &res[0], &res[1]);
                break :blk .{
                    .width = std.math.clamp(@as(u32, @intCast(res[0])), capabilities.min_image_extent.width, capabilities.max_image_extent.width),
                    .height = std.math.clamp(@as(u32, @intCast(res[1])), capabilities.min_image_extent.height, capabilities.max_image_extent.height),
                };
            }
            break :blk .{
                .width = std.math.clamp(capabilities.current_extent.width, capabilities.min_image_extent.width, capabilities.max_image_extent.width),
                .height = std.math.clamp(capabilities.current_extent.height, capabilities.min_image_extent.height, capabilities.max_image_extent.height),
            };
        };

        result.surface_format = blk: {
            const max_surface_format_count = 100;
            var surface_format_count: u32 = 0;
            _ = try instance.getPhysicalDeviceSurfaceFormatsKHR(pd().handle, result.surface, &surface_format_count, null);
            surface_format_count = @min(max_surface_format_count, surface_format_count);
            var surface_format_storage: [max_surface_format_count]vk.SurfaceFormatKHR = undefined;
            _ = try instance.getPhysicalDeviceSurfaceFormatsKHR(pd().handle, result.surface, &surface_format_count, &surface_format_storage);
            break :blk for (surface_format_storage[0..surface_format_count]) |sf| {
                if (sf.format == .b8g8r8a8_unorm and sf.color_space == .srgb_nonlinear_khr) break sf;
            } else surface_format_storage[0];
        };

        result.present_mode = blk: {
            var present_mode_storage: [@typeInfo(vk.PresentModeKHR).@"enum".fields.len]vk.PresentModeKHR = undefined;
            var present_mode_count: u32 = 0;
            _ = try instance.getPhysicalDeviceSurfacePresentModesKHR(pd().handle, result.surface, &present_mode_count, null);
            _ = try instance.getPhysicalDeviceSurfacePresentModesKHR(pd().handle, result.surface, &present_mode_count, &present_mode_storage);
            break :blk for (present_mode_storage[0..present_mode_count]) |pm| {
                if (pm == vk.PresentModeKHR.mailbox_khr) break pm;
            } else vk.PresentModeKHR.fifo_khr;
        };

        result.image_count = @max(capabilities.min_image_count, @as(u32, if (result.present_mode == .mailbox_khr) 3 else 2));
        const exclusive = pd().queueFamilyIndex(.graphics) == pd().queueFamilyIndex(.presentation);
        result.swapchain = try device.createSwapchainKHR(&.{
            .surface = result.surface,
            .min_image_count = result.image_count,

            .present_mode = result.present_mode,
            .image_format = result.surface_format.format,
            .image_color_space = result.surface_format.color_space,
            .image_extent = result.extent,

            .image_array_layers = 1,
            .image_sharing_mode = if (exclusive) .exclusive else .concurrent,
            .image_usage = .{
                .transfer_dst_bit = true,
                .color_attachment_bit = true,
            },
            .queue_family_index_count = if (exclusive) 0 else 2,
            .p_queue_family_indices = if (exclusive) null else &.{
                pd().queueFamilyIndex(.graphics),
                pd().queueFamilyIndex(.presentation),
            },
            .pre_transform = capabilities.current_transform,
            .composite_alpha = .{ .opaque_bit_khr = true },
            .clipped = .true,
        }, vka);

        _ = try device.getSwapchainImagesKHR(result.swapchain, &result.image_count, null);
        result.image_count = @min(result.image_count, mic);
        _ = try device.getSwapchainImagesKHR(result.swapchain, &result.image_count, &result.images);

        for (0..result.image_count) |i|
            result.image_views[i] = try device.createImageView(&.{
                .components = .{ .r = .identity, .g = .identity, .b = .identity, .a = .identity },
                .format = result.surface_format.format,
                .image = result.images[i],
                .subresource_range = .{
                    .aspect_mask = .{ .color_bit = true },
                    .layer_count = 1,
                    .base_array_layer = 0,
                    .level_count = 1,
                    .base_mip_level = 0,
                },
                .view_type = .@"2d",
            }, vka);

        for (0..result.fif()) |i| {
            result.acquire_semaphores[i] = try device.createSemaphore(&.{}, vka);
            result.fences[i] = try device.createFence(&.{ .flags = .{ .signaled_bit = true } }, vka);
        }
        for (0..result.image_count) |i|
            result.submit_semaphores[i] = try device.createSemaphore(&.{}, vka);
        return result;
    }
    pub fn destroy(self: *VKWindowContext, handle: ?WindowContext) void {
        if (handle != null and current_render_target == getWindowContextRenderTarget(handle.?)) {
            self.forceEndRendering();
        }
        device.queueWaitIdle(queue(.presentation)) catch {};

        for (self.image_views[0..self.image_count]) |iw|
            device.destroyImageView(iw, vka);
        device.destroySwapchainKHR(self.swapchain, vka);

        for (0..self.fif()) |i| {
            device.destroySemaphore(self.acquire_semaphores[i], vka);
            device.destroyFence(self.fences[i], vka);
        }
        for (0..self.image_count) |i|
            device.destroySemaphore(self.submit_semaphores[i], vka);

        instance.destroySurfaceKHR(self.surface, vka);
        self.* = .{};
    }
    pub fn get(handle: WindowContext) *VKWindowContext {
        return if (@intFromEnum(handle) == 0)
            &window_context_primary
        else
            @panic("");
    }
};

fn commandPool(queue_type: QueueType) Error!vk.CommandPool {
    const index = @intFromEnum(queue_type);
    if (command_pools[index] == .null_handle) {
        const qfi = pd().queueFamilyIndex(queue_type);
        const created = device.createCommandPool(&.{
            .flags = .{ .reset_command_buffer_bit = true },
            .queue_family_index = qfi,
        }, vka) catch
            return Error.ResourceCreationError;
        for (&command_pools, 0..) |*cmd_pools, i| {
            if (pd().queueFamilyIndex(@enumFromInt(i)) == qfi)
                cmd_pools.* = created;
        }
    }
    return command_pools[index];
}
fn allocCommandBuffer(queue_type: QueueType, level: vk.CommandBufferLevel) Error!vk.CommandBuffer {
    var result: vk.CommandBuffer = .null_handle;
    device.allocateCommandBuffers(&.{
        .command_pool = try commandPool(queue_type),
        .level = level,
        .command_buffer_count = 1,
    }, @ptrCast(&result)) catch return Error.ResourceCreationError;
    return result;
}
//===|vkextensions|====

fn isDynamicRenderingBuiltin() bool {
    return api_version.@">="(.{ .major = 1, .minor = 3 });
}
fn cmdBeginRendering(command_buffer: vk.CommandBuffer, rendering_info: *const vk.RenderingInfo) void {
    if (isDynamicRenderingBuiltin())
        device.cmdBeginRendering(command_buffer, rendering_info)
    else
        device.cmdBeginRenderingKHR(command_buffer, rendering_info);
}
fn cmdEndRendering(command_buffer: vk.CommandBuffer, rendering_info: *const vk.RenderingInfo) void {
    if (isDynamicRenderingBuiltin())
        device.cmdEndRendering(command_buffer, rendering_info)
    else
        device.cmdEndRenderingKHR(command_buffer, rendering_info);
}

//===|initialization|====

pub fn initBackend() VKError!gpu.Backend {
    bwp = .load(loader);
    arena = .init(std.heap.page_allocator);

    const instance_api_version = castVersion(@bitCast(bwp.enumerateInstanceVersion() catch return error.OutOfMemory));
    if (!instance_api_version.@">="(min_vulkan_version))
        return VKError.UnsupportedApiVersion;
    try initInstance(arena.allocator(), instance_api_version);

    var extension_name_buf: [10][*:0]const u8 = undefined;
    var device_extension_list: List([*:0]const u8) = .initBuffer(&extension_name_buf);

    device_extension_list.appendAssumeCapacity(vk.extensions.khr_swapchain.name);
    if (!instance_api_version.@">="(.{ .major = 1, .minor = 3 }))
        device_extension_list.appendAssumeCapacity(vk.extensions.khr_dynamic_rendering.name);

    try initPhysicalDevices(arena.allocator(), device_extension_list.items);
    const physical_device_api_version = castVersion(@bitCast(instance.getPhysicalDeviceProperties(pd().handle).api_version));

    physical_device_memory_properties = instance.getPhysicalDeviceMemoryProperties(pd().handle);
    api_version = if (physical_device_api_version.@">="(instance_api_version)) instance_api_version else physical_device_api_version;
    try initLogicalDeviceAndQueues(device_extension_list.items);

    shader_compiler = .new(null, null, .{
        .target_env = .vulkan1_4,
        .max_push_constant_buffer_size = max_push_constant_bytes,
    });
    return versionBackend(api_version);
}

fn deinit() void {
    defer if (arena.state.end_index != 0) {
        _ = arena.deinit();
        arena.state.end_index = 0;
    };
    endRendering() catch {};

    device.deviceWaitIdle() catch {};
    shader_compiler.deinit();

    const let_os_deinit = false;
    if (let_os_deinit) return;

    for (&command_pools) |cmd_pool| {
        for (&command_pools) |*i| {
            if (cmd_pool == i.*) i.* = .null_handle;
        }
        device.destroyCommandPool(cmd_pool, vka);
    }
    for (shader_module_list.items) |*i| i.destroy();
    shader_module_list.items = &.{};
    for (pipeline_list.items) |*i| i.destroy();
    pipeline_list.items = &.{};
    for (buffer_list.items) |*i| i.destroy();
    buffer_list.items = &.{};
    for (texture_list.items) |*i| i.destroy();
    texture_list.items = &.{};
    for (memory_allocation_list.items) |*i| i.free();
    memory_allocation_list.items = &.{};
    for (render_target_list.items) |*i| i.destroy();
    render_target_list.items = &.{};
    for (window_context_list.items) |*i| i.destroy(null);
    window_context_list.items = &.{};
    window_context_primary.destroy(null);

    device.destroyDevice(vka);
    instance.destroyInstance(vka);
}
fn initLogicalDeviceAndQueues(extensions: []const [*:0]const u8) VKError!void {
    var queue_create_infos: [queue_type_count]vk.DeviceQueueCreateInfo = undefined;
    var queues_to_create: [queue_type_count]u8 = undefined;
    var count: usize = 0;

    //track all unique queue families
    for (pd().queue_family_indices) |qi| {
        if (~qi == 0) continue;

        for (0..count) |c| {
            if (queues_to_create[c] == qi) break;
        } else {
            queues_to_create[count] = qi;
            queue_create_infos[count] = .{
                .queue_count = 1,
                .queue_family_index = qi,
                .p_queue_priorities = &.{1.0},
            };
            count += 1;
        }
    }

    const dynamic_rendering_feature_ptr: *const anyopaque =
        if (isDynamicRenderingBuiltin())
            &vk.PhysicalDeviceDynamicRenderingFeatures{ .dynamic_rendering = .true }
        else
            &vk.PhysicalDeviceDynamicRenderingFeaturesKHR{ .dynamic_rendering = .true };

    const device_create_info: vk.DeviceCreateInfo = .{
        .enabled_extension_count = @intCast(extensions.len),
        .pp_enabled_extension_names = extensions.ptr,
        .queue_create_info_count = @intCast(count),
        .p_queue_create_infos = &queue_create_infos,
        .pp_enabled_layer_names = layers.ptr,
        .enabled_layer_count = @intCast(layers.len),

        .p_next = dynamic_rendering_feature_ptr,
    };

    const device_handle = instance.createDevice(
        pd().handle,
        &device_create_info,
        vka,
    ) catch return VKError.LogicalDeviceInitializationFailure;
    dwp = .load(device_handle, instance.wrapper.dispatch.vkGetDeviceProcAddr.?);
    device = .init(device_handle, &dwp);

    for (0..queue_type_count) |i| {
        if (~pd().queue_family_indices[i] != 0)
            queues[i] = device.getDeviceQueue(pd().queue_family_indices[i], 0);
    }
}
fn initInstance(allocator: Allocator, instance_api_version: Version) VKError!void {
    const available_layers: []vk.LayerProperties =
        if (layers.len > 0) bwp.enumerateInstanceLayerPropertiesAlloc(allocator) catch return VKError.OutOfMemory else &.{};

    try checkLayerPresence(layers, available_layers);
    if (layers.len > 0) allocator.free(available_layers);

    var glfw_ext_count: u32 = 0; //get platform presentation extensions
    const glfw_exts = glfw.getRequiredInstanceExtensions(&glfw_ext_count);
    const instance_extensions: []const [*:0]const u8 = if (glfw_exts) |ge| ge[0..glfw_ext_count] else &.{};

    const available_instance_extensions: []vk.ExtensionProperties =
        bwp.enumerateInstanceExtensionPropertiesAlloc(null, allocator) catch return VKError.OutOfMemory;

    try checkExtensionPresence(instance_extensions, available_instance_extensions);
    allocator.free(available_instance_extensions);

    const app_info: vk.ApplicationInfo = .{
        .p_application_name = huge.name ++ " app",
        .application_version = @bitCast(@as(u32, 0)),
        .p_engine_name = huge.name,
        .engine_version = toVulkanVersion(huge.version),
        .api_version = toVulkanVersion(instance_api_version),
    };
    const instance_create_info: vk.InstanceCreateInfo = .{
        .p_application_info = &app_info,
        .enabled_extension_count = @intCast(instance_extensions.len),
        .pp_enabled_extension_names = instance_extensions.ptr,
        .enabled_layer_count = @intCast(layers.len),
        .pp_enabled_layer_names = layers.ptr,
    };
    const instance_handle = bwp.createInstance(&instance_create_info, vka) catch return VKError.InstanceInitializationFailure;

    iwp = .load(instance_handle, loader);
    instance = .init(instance_handle, &iwp);
}
fn initPhysicalDevices(allocator: Allocator, extensions: []const [*:0]const u8) VKError!void {
    var count: u32 = 0;
    _ = instance.enumeratePhysicalDevices(&count, null) catch
        return VKError.PhysicalDeviceInitializationFailure;

    if (count == 0) return error.PhysicalDeviceInitializationFailure;

    count = @min(physical_devices.len, count);
    var physical_device_handles: [physical_devices.len]vk.PhysicalDevice = undefined;
    _ = instance.enumeratePhysicalDevices(&count, &physical_device_handles) catch
        return VKError.PhysicalDeviceInitializationFailure;
    for (&physical_devices, &physical_device_handles) |*p, *ph| p.handle = ph.*;

    // create dummy window to use its surface
    // for physical device initialization
    const dummy_window = huge.Window.createDummy(@intFromEnum(instance.handle)) catch
        return VKError.DummyWindowCreationFailure;
    defer {
        instance.destroySurfaceKHR(@enumFromInt(dummy_window.surface_handle), vka);
        glfw.destroyWindow(dummy_window.handle);
    }

    valid_physical_device_count = count;
    var i: usize = 0;
    while (i < valid_physical_device_count) : (i += 1)
        initPhysicalDevice(allocator, &physical_devices[i], extensions, dummy_window) catch {
            valid_physical_device_count -= 1;
            std.mem.swap(PhysicalDevice, &physical_devices[i], &physical_devices[valid_physical_device_count]);
            i -= 1;
            continue; //remove from the array if initializaiton failed
        };
    if (valid_physical_device_count == 0) return VKError.PhysicalDeviceInitializationFailure;

    var max_score: u32 = 0; //pick best physical device
    //add ability to overwrite current physical device index
    for (physical_devices[0..valid_physical_device_count], 0..) |p, index| {
        const score = scorePhysicalDevice(p);
        if (score > max_score) {
            current_physical_device_index = index;
            max_score = score;
        }
    }
    if (false) std.debug.print("Format: {}\n", .{
        getVulkanFormat(.depth16_stencil8, .{
            .sampled = true,
            .sampled_linear = true,
            .sampled_minmax = true,
            // .color_attachment = true,
            // .color_attachment_blend = true,
            .depth_stencil_attachment = true,
            // .blit_src = true,
            // .blit_dst = true,
            // .transfer_src = true,
            // .transfer_dst = true,
        }, .image_optimal),
    });
}

fn scorePhysicalDevice(physical_device: PhysicalDevice) u32 {
    var score: u32 = 0;
    score = switch (physical_device.type) {
        .discrete_gpu => 4000,
        .integrated_gpu => 3000,
        .virtual_gpu => 2000,
        .cpu => 1000,
        else => 1,
    };
    if (physical_device.features.geometry_shaders) score += 100;
    if (physical_device.features.tessellation_shaders) score += 100;
    return score;
}

fn initPhysicalDevice(allocator: Allocator, p: *PhysicalDevice, extensions: []const [*:0]const u8, dummy_window: huge.Window.DummyWindow) VKError!void {
    p.features = getPhysicalDeviceFeatures(p.handle);

    const properties = instance.getPhysicalDeviceProperties(p.handle); //limits?
    if (!castVersion(@bitCast(properties.api_version)).@">="(min_vulkan_version))
        return VKError.UnsupportedApiVersion;

    p.type = properties.device_type;
    p.name_len = @min(
        std.mem.len(@as([*:0]const u8, @ptrCast(@alignCast(&properties.device_name)))),
        PhysicalDevice.max_name_len,
    );
    @memcpy(p.name_storage[0..p.name_len], properties.device_name[0..p.name_len]);

    const available_extensions =
        instance.enumerateDeviceExtensionPropertiesAlloc(p.handle, null, allocator) catch return VKError.OutOfMemory;
    try checkExtensionPresence(extensions, available_extensions);
    allocator.free(available_extensions);

    p.queue_family_indices =
        try getQueueFamilyIndices(allocator, p.handle, dummy_window);
}
fn getQueueFamilyIndices(allocator: Allocator, handle: vk.PhysicalDevice, dummy_window: huge.Window.DummyWindow) VKError![queue_type_count]u8 {
    const queue_family_properties = instance.getPhysicalDeviceQueueFamilyPropertiesAlloc(handle, allocator) catch
        return VKError.OutOfMemory;
    defer allocator.free(queue_family_properties);

    var index_lists: [queue_type_count]IndexList = undefined;

    for (&index_lists) |*l| l.init();

    for (queue_family_properties, 0..) |qfp, i| {
        const flags: QueueConfiguration = .{
            .graphics = qfp.queue_flags.graphics_bit,
            .compute = qfp.queue_flags.compute_bit,
            .transfer = qfp.queue_flags.transfer_bit,
            .sparse_binding = qfp.queue_flags.sparse_binding_bit,
            .protected = qfp.queue_flags.protected_bit,
            .video_decode = qfp.queue_flags.video_decode_bit_khr,
            .video_encode = qfp.queue_flags.video_encode_bit_khr,
            .presentation = @intFromEnum(instance.getPhysicalDeviceSurfaceSupportKHR(handle, @intCast(i), @enumFromInt(dummy_window.surface_handle)) catch .false) > 0,
        };
        inline for (@typeInfo(QueueType).@"enum".fields, 0..) |ef, j|
            if (@field(flags, ef.name))
                index_lists[j].append(@intCast(i));
    }
    //check for minimal reqired queues
    var any_flags: QueueConfiguration = .{};
    inline for (@typeInfo(QueueType).@"enum".fields, 0..) |ef, i| {
        // check if there are any queue families
        // at the index corresponding to that queue
        @field(any_flags, ef.name) = index_lists[i].list.items.len > 0;
    }
    if (!util.matchFlagStructs(
        QueueConfiguration,
        any_flags,
        minimal_required_queue_family_config,
    )) return VKError.MissingQueueType;

    //iterate through all the possible queue configurations score them and use the best one
    var non_empty_index_storage: [queue_type_count]usize = undefined;
    var count: usize = 0;
    //use this to avoid iterating through queue families that have no available queue
    for (&index_lists, 0..) |*l, i| {
        if (l.list.items.len > 0) {
            non_empty_index_storage[count] = i;
            count += 1;
        }
    }
    var max_score: i32 = std.math.minInt(i32);
    var current_queue_family_indices: [queue_type_count]u8 = @splat(0xff);

    var queue_family_indices: [queue_type_count]u8 = @splat(0xff);
    findBestQueueConfiguration(
        &queue_family_indices,
        index_lists,
        non_empty_index_storage[0..count],
        &current_queue_family_indices,
        0,
        &max_score,
    );
    return queue_family_indices;
}

const IndexList = struct {
    list: std.ArrayList(u8),
    buf: [max_queue_family_count]u8,
    pub fn init(self: *IndexList) void {
        self.list = .initBuffer(&self.buf);
    }
    pub fn append(self: *IndexList, i: u8) void {
        self.list.appendAssumeCapacity(i);
    }
};
fn findBestQueueConfiguration(
    out: *[queue_type_count]u8,
    index_lists: [queue_type_count]IndexList,
    non_empty_indices: []usize,
    current: *[queue_type_count]u8,
    depth: usize,
    max_score: *i32,
) void {
    if (depth == non_empty_indices.len) {
        const score = scoreQueueConfiguration(current);
        if (score > max_score.*) {
            max_score.* = score;
            //copy the best into the global storage
            out.* = current.*;
        }
        return;
    }
    const index = non_empty_indices[depth];
    for (index_lists[index].list.items) |value| {
        current[index] = value;
        findBestQueueConfiguration(
            out,
            index_lists,
            non_empty_indices,
            current,
            depth + 1,
            max_score,
        );
    }
}
fn scoreQueueConfiguration(configuration: []u8) i32 {
    var score: i32 = 0;
    inline for (queueConfigurationScoringRules) |rule| {
        const values: [2]u8 = .{
            configuration[@intFromEnum(rule[1][0])],
            configuration[@intFromEnum(rule[1][1])],
        };
        if (values[0] == values[1] and ~values[0] != 0) score += rule[0];
    }
    return score;
}
fn getPhysicalDeviceFeatures(handle: vk.PhysicalDevice) gpu.FeatureSet {
    const vk_features = instance.getPhysicalDeviceFeatures(handle);
    return .{
        .geometry_shaders = vk_features.geometry_shader != .false,
        .tessellation_shaders = vk_features.tessellation_shader != .false,
        .shader_float64 = vk_features.shader_float_64 != .false,
        .shader_int64 = vk_features.shader_int_64 != .false,
        .shader_int16 = vk_features.shader_int_16 != .false,
    };
}
const PhysicalDevice = struct {
    handle: vk.PhysicalDevice = .null_handle,
    queue_family_indices: [queue_type_count]u8 = @splat(0xff),
    name_storage: [max_name_len]u8 = @splat(0),
    name_len: usize = 0,
    features: gpu.FeatureSet = .{},
    type: vk.PhysicalDeviceType = .discrete_gpu,

    pub const max_name_len = 128;
    pub fn queueFamilyIndex(self: *const PhysicalDevice, queue_type: QueueType) u8 {
        return self.queue_family_indices[@intFromEnum(queue_type)];
    }
    pub fn format(self: PhysicalDevice, writer: *std.Io.Writer) !void {
        try writer.print("Physical Device({}){{\n", .{self.handle});
        try writer.print("Name: {s}\n", .{self.name_storage[0..self.name_len]});
        try writer.print("Family Queue Indices: {any}\n", .{self.queue_family_indices});
        try writer.print("Type: {}\n", .{self.type});
        try writer.print("Features: {{\n", .{});
        inline for (@typeInfo(gpu.Feature).@"enum".fields) |ef|
            try writer.print("\t{s} = {}\n", .{ ef.name, @field(self.features, ef.name) });
        try writer.print("}}", .{});
    }
};
fn checkExtensionPresence(required: []const [*:0]const u8, available: []const vk.ExtensionProperties) VKError!void {
    for (required) |re| { //chech for unavailable instance extensions
        if (!for (available) |ae| {
            if (huge.util.strEqlNullTerm(re, @ptrCast(@alignCast(&ae.extension_name)))) break true;
        } else false) return VKError.UnavailableExtension; //TODO: log missing extension name
    }
}
fn checkLayerPresence(required: []const [*:0]const u8, available: []const vk.LayerProperties) VKError!void {
    for (required) |rl| { //chech for unavailable instance extensions
        if (!for (available) |al| {
            if (huge.util.strEqlNullTerm(rl, @ptrCast(@alignCast(&al.layer_name)))) break true;
        } else false) return VKError.UnavailableLayer; //TODO: log missing layer name
    }
}

//=======================

pub const queue_type_count = util.enumLen(QueueType);
const QueueConfiguration = util.StructFromEnum(QueueType, bool, false, .@"packed");
const queueConfigurationScoringRules: []const std.meta.Tuple(&.{ i32, [2]QueueType }) = &.{
    .{ -150, .{ .graphics, .compute } },
    .{ -150, .{ .graphics, .transfer } },
    .{ 100, .{ .graphics, .presentation } },
    .{ -90, .{ .compute, .transfer } },
    .{ 30, .{ .sparse_binding, .transfer } },
};
pub const minimal_required_queue_family_config: QueueConfiguration = .{
    .graphics = true,
    .presentation = true,
    .transfer = true,
    .compute = true,
};
pub const QueueType = enum(u8) { graphics, presentation, compute, transfer, sparse_binding, protected, video_decode, video_encode };

//=======================
fn getStageFlags(stage_info: hgsl.StageInfo) vk.ShaderStageFlags {
    return .{
        .vertex_bit = stage_info == .vertex,
        .fragment_bit = stage_info == .fragment,
    };
}
fn castPrimitiveTopology(primitive: gpu.PrimitiveTopology) vk.PrimitiveTopology {
    return switch (primitive) {
        .triangle => .triangle_list,
        .triangle_strip => .triangle_strip,
        .triangle_fan => .triangle_fan,
        .line => .line_list,
        .line_strip => .line_strip,
        .point => .point_list,
    };
}
pub const loader = &struct {
    pub fn l(i: vk.Instance, name: [*:0]const u8) ?glfw.VKproc {
        return glfw.getInstanceProcAddress(@intFromEnum(i), name);
    }
}.l;
fn castVersion(vk_version: vk.Version) Version {
    return .{
        .major = vk_version.major,
        .minor = vk_version.minor,
    };
}
fn toVulkanVersion(version: Version) u32 {
    return @bitCast(vk.makeApiVersion(0, @truncate(version.major), @truncate(version.minor), 0));
}
const glfw = huge.Window.glfw;
const Error = gpu.Error;
const Pipeline = gpu.Pipeline;
const Format = gpu.Format;
const ShaderModule = gpu.ShaderModule;
const RenderTarget = gpu.RenderTarget;
const Texture = gpu.Texture;
const Buffer = gpu.Buffer;
const BufferUsage = gpu.BufferUsage;
const WindowContext = gpu.WindowContext;
const ClearValue = gpu.ClearValue;
const Version = huge.Version;
const Allocator = std.mem.Allocator;
const List = std.ArrayList;
const FormatUsage = packed struct(u11) {
    //image
    vertex: bool = false,
    sampled: bool = false,
    sampled_linear: bool = false,
    sampled_minmax: bool = false,

    color_attachment: bool = false,
    color_attachment_blend: bool = false,
    depth_stencil_attachment: bool = false,

    blit_src: bool = false,
    blit_dst: bool = false,

    transfer_src: bool = false,
    transfer_dst: bool = false,
    //video decode/encode
    pub fn toImageUsage(self: FormatUsage) vk.ImageUsageFlags {
        return .{
            .transfer_src_bit = self.transfer_src,
            .transfer_dst_bit = self.transfer_dst,
            .sampled_bit = self.sampled,
            .storage_bit = false, // !
            .color_attachment_bit = self.color_attachment,
            .depth_stencil_attachment_bit = self.depth_stencil_attachment,
        };
    }
};
fn formatFromIOType(io_type: hgsl.IOType) vk.Format {
    return if (io_type == .scalar) switch (io_type.scalar) {
        .f32 => .r32_sfloat,
        .i32 => .r32_sint,
        .u32 => .r32_uint,
        .f64 => .r64_sfloat,
        .i64 => .r64_sint,
        .u64 => .r64_uint,
    } else switch (io_type.vector.len) {
        ._2 => switch (io_type.vector.child) {
            .f32 => .r32g32_sfloat,
            .i32 => .r32g32_sint,
            .u32 => .r32g32_uint,
            .f64 => .r64g64_sfloat,
            .i64 => .r64g64_sint,
            .u64 => .r64g64_uint,
        },
        ._3 => switch (io_type.vector.child) {
            .f32 => .r32g32b32_sfloat,
            .i32 => .r32g32b32_sint,
            .u32 => .r32g32b32_uint,
            .f64 => .r64g64b64_sfloat,
            .i64 => .r64g64b64_sint,
            .u64 => .r64g64b64_uint,
        },
        ._4 => switch (io_type.vector.child) {
            .f32 => .r32g32b32a32_sfloat,
            .i32 => .r32g32b32a32_sint,
            .u32 => .r32g32b32a32_uint,
            .f64 => .r64g64b64a64_sfloat,
            .i64 => .r64g64b64a64_sint,
            .u64 => .r64g64b64a64_uint,
        },
    };
}

const FormatUsageLayout = enum {
    image_linear,
    image_optimal,
    buffer,
};
fn getVulkanFormat(format: Format, usage: FormatUsage, layout: FormatUsageLayout) vk.Format {
    return for (frmt.compatibleFormats(format)) |vkf| {
        const properties = instance.getPhysicalDeviceFormatProperties(pd().handle, vkf);
        const flags: vk.FormatFeatureFlags = switch (layout) {
            .image_linear => properties.linear_tiling_features,
            .image_optimal => properties.optimal_tiling_features,
            .buffer => properties.buffer_features,
        };
        const valid = (!usage.vertex or flags.vertex_buffer_bit) and
            (!usage.sampled or flags.sampled_image_bit) and
            (!usage.sampled_linear or flags.sampled_image_filter_linear_bit) and
            (!usage.sampled_minmax or flags.sampled_image_filter_minmax_bit) and
            (!usage.color_attachment or flags.color_attachment_bit) and
            (!usage.color_attachment_blend or flags.color_attachment_blend_bit) and
            (!usage.depth_stencil_attachment or flags.depth_stencil_attachment_bit) and
            (!usage.blit_src or flags.blit_src_bit) and
            (!usage.blit_dst or flags.blit_dst_bit) and
            (!usage.transfer_src or flags.transfer_src_bit) and
            (!usage.transfer_dst or flags.transfer_dst_bit);
        if (valid) break vkf;
    } else .undefined;
}
const VKError = error{
    OutOfMemory,

    UnavailableExtension,
    UnsupportedApiVersion,
    UnavailableLayer,

    InstanceInitializationFailure,
    PhysicalDeviceInitializationFailure,
    DummyWindowCreationFailure,
    MissingQueueType,

    LogicalDeviceInitializationFailure,
};
fn versionBackend(version: Version) gpu.Backend {
    return .{
        .api = .vulkan,
        .api_version = version,
        .deinit = &deinit,

        .draw = &draw,
        .bindVertexBuffer = &bindVertexBuffer,
        .bindIndexBuffer = &bindIndexBuffer,
        .beginRendering = &beginRendering,
        .endRendering = &endRendering,

        .reloadPipelines = &reloadPipelines,
        .pipelinePushConstant = pipelinePushConstant,
        .createPipeline = &createPipeline,
        .createShaderModulePath = &createShaderModulePath,
        .destroyShaderModule = &destroyShaderModule,

        .renderTargetSize = &renderTargetSize,
        .createRenderTargetFromTextures = &createRenderTargetFromTextures,
        .createRenderTarget = &createRenderTarget,
        .destroyRenderTarget = &destroyRenderTarget,

        .getTextureType = &getTextureType,
        .getTextureSize = &getTextureSize,
        .getTextureFormat = &getTextureFormat,
        .createTexture = &createTexture,
        .destroyTexture = &destroyTexture,

        .loadBuffer = &loadBuffer,
        .mapBuffer = &mapBuffer,
        .unmapBuffer = &unmapBuffer,
        .createBuffer = &createBuffer,
        .destroyBuffer = &destroyBuffer,

        .updateWindowContext = &updateWindowContext,
        .getWindowRenderTarget = &getWindowRenderTarget,
        .createWindowContext = &createWindowContext,
        .destroyWindowContext = &destroyWindowContext,
    };
}
