const std = @import("std");
const zigbuiltin = @import("builtin");
const huge = @import("../../root.zig");
const math = huge.math;
const util = huge.util;
const gpu = huge.gpu;
const hgsl = gpu.hgsl;

const VKTexture = @import("VKTexture.zig");

const frmt = @import("format.zig");
//generated by: https://github.com/Snektron/vulkan-zig
const vk = @import("vk.zig");

//=====|constants|======

pub const u32m = ~@as(u32, 0);
const timeout: u64 = std.time.ns_per_s * 5;
const max_queue_family_count = 16;

const min_vulkan_version: Version = .{ .major = 1, .minor = 2 };
const max_vulkan_version: Version = undefined;

const max_push_constant_bytes = 128;
const layers: []const [*:0]const u8 = if (zigbuiltin.mode == .Debug) &.{
    "VK_LAYER_KHRONOS_validation",
    // "VK_LAYER_LUNARG_api_dump",
} else &.{};

//=======|state|========

pub var vka: ?*vk.AllocationCallbacks = null;

var instance: vk.InstanceProxy = undefined;
var physical_devices: [3]PhysicalDevice = @splat(.{});
var valid_physical_device_count: usize = 0;
var current_physical_device_index: usize = 0;
pub var device: vk.DeviceProxy = undefined;

var bwp: vk.BaseWrapper = undefined;
var iwp: vk.InstanceWrapper = undefined;
var dwp: vk.DeviceWrapper = undefined;
var queues: [queue_type_count]vk.Queue = @splat(.null_handle);
inline fn queue(queue_type: QueueType) vk.Queue {
    return queues[@intFromEnum(queue_type)];
}

var command_pools: [queue_type_count]vk.CommandPool = @splat(.null_handle);

inline fn pd() PhysicalDevice {
    return physical_devices[current_physical_device_index];
}
var physical_device_memory_properties: vk.PhysicalDeviceMemoryProperties = undefined;

var api_version: Version = undefined;
pub var arena: std.heap.ArenaAllocator = undefined;
var shader_compiler: hgsl.Compiler = undefined;
// var pipelines: List(VKPipeline) = .empty;

pub var shader_module_list: List(VKShaderModule) = .empty;
pub var pipeline_list: List(VKPipeline) = .empty;
pub var buffer_list: List(VKBuffer) = .empty;
pub var texture_list: List(VKTexture) = .empty;
pub var memory_allocation_list: List(MemoryAllocation) = .empty;
pub var render_target_list: List(VKRenderTarget) = .empty;

var window_context_primary: VKWindowContext = .{};
var window_context_list: List(VKWindowContext) = .empty;
var window_context_count: u32 = 0;

const null_render_target: RenderTarget = @enumFromInt(~@as(gpu.Handle, 0));
var current_render_target: RenderTarget = null_render_target;
const MemoryAllocation = struct {
    memory: vk.DeviceMemory = .null_handle,
    ref_count: gpu.Handle = 0,
    pub fn free(self: *MemoryAllocation) void {
        if (self.memory != .null_handle)
            device.freeMemory(self.memory, vka);
        self.memory = .null_handle;
    }
};

var descriptor_pools: [10]vk.DescriptorPool = @splat(.null_handle);
//======|methods|========

pub fn updateBuffer(handle: Buffer, bytes: []const u8, offset: usize) void {
    const cmd = getDrawCommandCmd(.graphics);
    if (cmd == .null_handle) return;

    const buffer = VKBuffer.get(handle);
    device.cmdUpdateBuffer(
        cmd,
        buffer.vk_handle,
        offset,
        @min(bytes.len, buffer.size - @as(u64, @intCast(offset))),
        bytes.ptr,
    );
}
pub fn draw(handle: Pipeline, params: gpu.DrawParams) void {
    const cmd = getDrawCommandCmd(.graphics);
    if (cmd == .null_handle) return;

    VKPipeline.get(handle).bind(cmd, .graphics);

    if (params.indexed_vertex_offset) |vo| {
        device.cmdDrawIndexed(
            cmd,
            params.count,
            params.instance_count,
            params.offset,
            vo,
            params.instance_offset,
        );
    } else device.cmdDraw(
        cmd,
        params.count,
        params.instance_count,
        params.offset,
        params.instance_offset,
    );
}
pub fn bindIndexBuffer(handle: Buffer, index_type: gpu.IndexType) void {
    const cmd = getDrawCommandCmd(.graphics);
    if (cmd == .null_handle) return;

    const buffer: *VKBuffer = .get(handle);
    huge.dassert(buffer.usage == .index);
    if (buffer.usage != .index) return;

    if (api_version.minor < 4 and index_type == .u8)
        @panic("TODO: index_type u8 extension");
    device.cmdBindIndexBuffer(cmd, buffer.vk_handle, 0, switch (index_type) {
        .u32 => .uint32,
        .u16 => .uint16,
        .u8 => .uint8, //only in core spec from 1_4
    });
}
pub fn bindVertexBuffer(handle: Buffer) void {
    const cmd = getDrawCommandCmd(.graphics);
    if (cmd == .null_handle) return;

    const buffer: *VKBuffer = .get(handle);
    huge.dassert(buffer.usage == .vertex);
    if (buffer.usage != .vertex) return;

    device.cmdBindVertexBuffers(cmd, 0, 1, &.{buffer.vk_handle}, &.{0});
}

pub fn beginRendering(render_target: RenderTarget, clear_value: ClearValue) Error!void {
    try endRendering();

    current_render_target = render_target;
    if (VKWindowContext.fromRenderTarget(render_target)) |wc| {
        try wc.beginRenderingToWindow(clear_value);
    } else @panic("begin rendering NOT to WINDOW");
}
pub fn endRendering() Error!void {
    if (current_render_target == null_render_target) return;
    defer current_render_target = null_render_target;

    if (VKWindowContext.fromRenderTarget(current_render_target)) |wc| {
        try wc.endRenderingToWindow();
    } else @panic("end rendering NOT to WINDOW");
}

pub fn forceEndRendering() void {
    if (current_render_target == null_render_target) return;
    defer current_render_target = null_render_target;

    if (VKWindowContext.fromRenderTarget(current_render_target)) |wc| {
        wc.forceEndRendering();
    } else @panic("end rendering URGENT NOT to WINDOW");
}

pub fn reloadPipelines() Error!void {
    device.deviceWaitIdle() catch return;
    for (shader_module_list.items) |*sm| sm.load_state = .unloaded;
    for (pipeline_list.items) |*p| {
        for (p.shader_modules[0..p.stage_count]) |s| {
            VKShaderModule.get(s).recreate();
        }
        try p.recreate();
    }

    forceEndRendering();
}
// setPipelineOpaqueUniform: pub SetPipelineOpaqueUniformFn = undefined,
pub fn pipelineSetOpaqueUniform(
    handle: Pipeline,
    name: []const u8,
    local_offset: u32,
    local_size: u32,
    opaque_type: gpu.OpaqueType,
    uniform_handle: gpu.Handle,
) void {
    _ = .{ local_offset, local_size };
    const cmd = getDrawCommandCmd(.graphics);
    if (cmd == .null_handle) return;

    const pipeline: *VKPipeline = .get(handle);

    var bindings: [Pipeline.max_pipeline_stages]u32 = undefined;
    var binding_count: usize = 0;
    for (pipeline.entry_point_info_storage[0..pipeline.stage_count]) |ep_info| {
        ouloop: for (ep_info.opaque_uniform_mappings) |ou| {
            if (util.strEql(name, ou.name)) {
                if (getDescriptorTypeHGSL(ou.type) != getDescriptorType(opaque_type, uniform_handle))
                    continue;
                //dont repeat cmdPushConstants call with the same offset
                for (bindings[0..binding_count]) |b| if (b == ou.binding) continue :ouloop;

                bindings[binding_count] = ou.binding;
                binding_count += 1;
                const ptr: *const anyopaque = switch (opaque_type) {
                    .buffer => blk: {
                        const buffer = VKBuffer.get(@enumFromInt(uniform_handle));
                        break :blk @ptrCast(@alignCast(&vk.DescriptorBufferInfo{
                            .buffer = buffer.vk_handle,
                            .offset = 0,
                            .range = buffer.size,
                        }));
                    },
                    .texture => blk: {
                        const texture = VKTexture.get(@enumFromInt(uniform_handle));
                        if (texture.sampler != .null_handle)
                            device.cmdPipelineBarrier(cmd, .{
                                .fragment_shader_bit = true,
                            }, .{
                                .fragment_shader_bit = true,
                            }, .{}, 0, null, 0, null, 1, &.{.{
                                .src_access_mask = .{ .color_attachment_write_bit = true },
                                .dst_access_mask = .{},
                                .old_layout = .undefined,
                                .new_layout = texture.samplingLayout(),
                                .src_queue_family_index = pd().queueFamilyIndex(.presentation),
                                .dst_queue_family_index = pd().queueFamilyIndex(.presentation),
                                .image = texture.image,
                                .subresource_range = .{
                                    .aspect_mask = .{
                                        .color_bit = !texture.format.isDepthStencil(),
                                        .depth_bit = false,
                                        .stencil_bit = false,
                                    },
                                    .base_mip_level = 0,
                                    .level_count = 1,
                                    .base_array_layer = 0,
                                    .layer_count = 1,
                                },
                            }});

                        break :blk @ptrCast(@alignCast(&vk.DescriptorImageInfo{
                            .sampler = texture.sampler,
                            .image_view = texture.view,
                            .image_layout = texture.samplingLayout(),
                            // sampler: Sampler,
                            // image_view: ImageView,
                            // image_layout: ImageLayout,
                        }));
                    },
                    // else => @panic("idk this opaque uniform descriptor"),
                };

                //update multiple at a time
                device.updateDescriptorSets(1, &.{.{
                    .dst_set = pipeline.descriptor_set,
                    .dst_binding = ou.binding,
                    .dst_array_element = local_offset,
                    .descriptor_count = 1,
                    .descriptor_type = getDescriptorTypeHGSL(ou.type),
                    .p_image_info = if (opaque_type == .texture)
                        @ptrCast(@alignCast(ptr))
                    else
                        &.{},
                    .p_buffer_info = if (opaque_type == .buffer)
                        @ptrCast(@alignCast(ptr))
                    else
                        &.{},
                    .p_texel_buffer_view = &.{},
                }}, 0, null);
            }
        }
        //log if didnt found
    }
}
pub fn pipelinePushConstant(
    handle: Pipeline,
    name: []const u8,
    local_offset: u32,
    local_size: u32,
    ptr: *const anyopaque,
) void {
    _ = .{ local_offset, local_size };
    const cmd = getDrawCommandCmd(.graphics);
    if (cmd == .null_handle) return;

    const pipeline: *VKPipeline = .get(handle);

    var offsets: [Pipeline.max_pipeline_stages]u32 = undefined;
    var offset_count: u32 = 0;
    for (pipeline.entry_point_info_storage[0..pipeline.stage_count]) |ep_info| {
        pcloop: for (ep_info.push_constant_mappings) |pc| {
            if (util.strEql(name, pc.name)) {
                if (pc.offset >= max_push_constant_bytes)
                    continue; //fully out of bounds

                //dont repeat cmdPushConstants call with the same offset
                for (offsets[0..offset_count]) |o| if (o == pc.offset) continue :pcloop;

                offsets[offset_count] = pc.offset;
                offset_count += 1;

                var stage_flags: vk.Flags = 0;
                for (pipeline.push_constant_ranges[0..pipeline.push_constant_range_count]) |r| {
                    if (r.size < pc.offset + pc.size) break;
                    stage_flags |= r.stage_flags.toInt();
                }

                device.cmdPushConstants(
                    cmd,
                    pipeline.layout,
                    @bitCast(stage_flags),
                    pc.offset,
                    pc.size - ((pc.offset + pc.size) -| max_push_constant_bytes),
                    ptr,
                );
            }
        }
        //log if didnt found
    }
}
pub fn createPipeline(stage_modules: []const ShaderModule, params: gpu.PipelineParams) Error!Pipeline {
    const handle: Pipeline = @enumFromInt(@as(gpu.Handle, @intCast(pipeline_list.items.len)));
    try pipeline_list.append(arena.allocator(), try VKPipeline.create(stage_modules, params));
    return handle;
}

pub fn createShaderModulePath(path: []const u8, entry_point: []const u8) Error!ShaderModule {
    const handle: ShaderModule = @enumFromInt(@as(gpu.Handle, @intCast(shader_module_list.items.len)));
    try shader_module_list.append(arena.allocator(), try VKShaderModule.createPath(path, entry_point));
    return handle;
}
pub fn destroyShaderModule(handle: ShaderModule) void {
    VKShaderModule.get(handle).destroy();
}
pub fn loadBuffer(handle: Buffer, bytes: []const u8, offset: usize) Error!void {
    const mapped = try mapBuffer(handle, bytes.len, offset);
    @memcpy(mapped, bytes[0..@min(bytes.len, mapped.len)]);
    defer unmapBuffer(handle);
}
pub fn mapBuffer(handle: Buffer, bytes: usize, offset: usize) Error![]u8 {
    const buffer: *VKBuffer = .get(handle);
    if (buffer.mapped) return Error.MemoryRemap;

    const ptr = (device.mapMemory(accessMemory(buffer.device_memory), offset, @intCast(bytes), .{}) catch
        return Error.OutOfMemory) orelse
        return Error.OutOfMemory;
    buffer.mapped = true;
    return @as([*]u8, @ptrCast(ptr))[0..@min(bytes, buffer.size)];
}
pub fn unmapBuffer(handle: Buffer) void {
    const buffer: *VKBuffer = .get(handle);
    if (!buffer.mapped) return;

    device.unmapMemory(accessMemory(buffer.device_memory));
    buffer.mapped = false;
}
pub fn createRenderTargetFromTextures(color: ?Texture, depth_stencil: ?Texture) Error!RenderTarget {
    var rt: VKRenderTarget = .{};
    if (color) |c| {
        rt.mask.color = true;
        rt.color_attachment = c;
    }
    if (depth_stencil) |d| {
        rt.mask.depth_stencil = true;
        rt.depth_stencil_attachment = d;
    }
    if (!rt.mask.color and !rt.mask.depth_stencil) return null_render_target;
    if (!rt.mask.color or !rt.mask.depth_stencil) {
        const t: *VKTexture = .get(if (color) |c| c else depth_stencil.?);
        _ = try VKTexture.recreateAsAttachment(@ptrCast(t));
        return try rt.append();
    }

    const ct, const dst = .{ VKTexture.get(color.?), VKTexture.get(depth_stencil.?) };

    // if (!std.meta.eql(ct.params, dst.params))
    //     return Error.NonMatchingRenderAttachmentParams;
    var arr: [2]VKTexture = .{ ct.*, dst.* };
    if (try VKTexture.recreateAsAttachment(&arr)) {
        ct.* = arr[0];
        dst.* = arr[1];
    }
    return try rt.append();
}
// renderTargetFromTextures: *const pub RendrerTargetFromTexturesFn = undefined,
pub fn createRenderTarget(
    size: math.uvec2,
    color_format: ?gpu.Format,
    depth_stencil_format: ?gpu.Format,
    sampling_options: ?gpu.SamplingOptions,
) Error!RenderTarget {
    if (color_format == null and depth_stencil_format == null) return null_render_target;
    const render_target = try VKRenderTarget.create(size, color_format, depth_stencil_format, sampling_options);
    return try render_target.append();
}
pub fn destroyRenderTarget(handle: RenderTarget) void {
    VKRenderTarget.get(handle).destroy();
}

// createRenderTarget: *const pub CreateRenderTargetFn = undefined,
// destroyRenderTarget: *const pub DestroyRenderTargetFn = undefined,

pub fn getTextureDimensions(handle: Texture) TextureDimensions {
    return VKTexture.get(handle).dimensions;
}
pub fn getTextureFormat(handle: Texture) Format {
    return VKTexture.get(handle).format;
}
pub fn createTexture(dimensions: TextureDimensions, format: Format, sampling_options: ?gpu.SamplingOptions) Error!Texture {
    const texture: VKTexture = try .create(dimensions, format, sampling_options, false);
    return try texture.append();
}
pub fn destroyTexture(handle: Texture) void {
    VKTexture.get(handle).destroy();
}
pub fn getBufferUsage(self: Buffer) gpu.BufferUsage {
    return VKBuffer.get(self).usage;
}

pub fn createBuffer(size: usize, buf_usage: BufferUsage) Error!Buffer {
    return try (try VKBuffer.create(@intCast(size), buf_usage)).append();
}
pub fn destroyBuffer(handle: Buffer) void {
    VKBuffer.get(handle).destroy();
}

pub fn renderTargetSize(render_target: RenderTarget) math.uvec2 {
    if (render_target == null_render_target) return @splat(0);

    return if (VKWindowContext.fromRenderTarget(render_target)) |wc|
        .{ wc.extent.width, wc.extent.height }
    else
        VKRenderTarget.get(render_target).size();
    // math.swizzle(VKTexture.fromRenderTarget(render_target).size, .xy);
}

pub fn updateWindowContext(handle: WindowContext) void {
    _ = handle;
}

pub fn getWindowRenderTarget(window: huge.Window) RenderTarget {
    return getWindowContextRenderTarget(window.context);
}
pub fn getWindowContextRenderTarget(handle: WindowContext) RenderTarget {
    return @enumFromInt((1 << 31) | @intFromEnum(handle));
}
pub fn createWindowContext(window: huge.Window) Error!WindowContext {
    const wc = VKWindowContext.create(window) catch
        return Error.WindowContextCreationError;
    if (window_context_count == 0) {
        window_context_primary = wc;
        return @enumFromInt(0);
    } else {
        @panic("VK multiple window contexts");
    }
}
pub fn destroyWindowContext(handle: WindowContext) void {
    const window_context = VKWindowContext.get(handle);
    window_context.destroy(handle);
}

//===|implementations|===
const VKShaderModule = struct {
    const LoadState = enum { unloaded, fail, success };
    load_state: LoadState = .unloaded,
    vk_handle: vk.ShaderModule = .null_handle,

    entry_point_info: hgsl.EntryPointInfo = .{},
    path: []const u8 = "",
    entry_point: []const u8 = "",

    pub fn recreate(self: *VKShaderModule) void {
        if (self.load_state == .unloaded) {
            const new = createPath(self.path, self.entry_point) catch {
                self.load_state = .fail;
                return;
            };
            device.destroyShaderModule(self.vk_handle, vka);
            self.* = new;
        }
    }

    pub fn createPath(path: []const u8, entry_point: []const u8) Error!VKShaderModule {
        const result = shader_compiler.compileFile(path) catch
            return Error.ShaderCompilationError;
        const vk_handle: vk.ShaderModule = device.createShaderModule(&.{
            .code_size = result.bytes.len,
            .p_code = @ptrCast(@alignCast(result.bytes.ptr)),
        }, vka) catch
            return Error.OutOfMemory;

        return .{
            .load_state = .success,
            .path = path,
            .entry_point = entry_point,
            .vk_handle = vk_handle,
            .entry_point_info = for (result.entry_point_infos) |ep| {
                if (util.strEql(entry_point, ep.name)) break ep;
            } else return Error.ShaderEntryPointNotFound,
        };
        // const result = shader_compiler.compileFile(path);
    }
    pub fn createSource(source: []const u8, entry_point: []const u8) Error!VKShaderModule {
        if (true) @panic("VKShaderModule.createRaw");
        return try createPath(source, entry_point);
    }
    pub fn destroy(self: *VKShaderModule) void {
        device.destroyShaderModule(self.vk_handle, vka);
        self.* = .{};
    }

    pub fn get(handle: ShaderModule) *VKShaderModule {
        return &shader_module_list.items[@intFromEnum(handle)];
    }
};
//handle array with functions that have explicit
//(offset and size) or (binding) args on top of 'name'
const VKPipeline = struct {
    vk_handle: vk.Pipeline = .null_handle,
    layout: vk.PipelineLayout = .null_handle,

    descriptor_set_layout: vk.DescriptorSetLayout = .null_handle,
    descriptor_set: vk.DescriptorSet = .null_handle,

    stage_count: u32 = 0,
    shader_modules: [Pipeline.max_pipeline_stages]ShaderModule = undefined,
    entry_point_info_storage: [Pipeline.max_pipeline_stages]hgsl.EntryPointInfo = undefined,
    push_constant_ranges: [Pipeline.max_pipeline_stages]vk.PushConstantRange = undefined,
    push_constant_range_count: u32 = 0,

    params: gpu.PipelineParams = .{},

    pub fn bind(self: *VKPipeline, cmd: vk.CommandBuffer, bind_point: vk.PipelineBindPoint) void {
        // vk.DescriptorSetAllocateInfo
        // if (descriptor_set != .null_handle)
        device.cmdBindDescriptorSets(cmd, bind_point, self.layout, 0, 1, &.{self.descriptor_set}, 0, null);
        device.cmdBindPipeline(cmd, bind_point, self.vk_handle);
    }
    pub fn recreate(self: *VKPipeline) Error!void {
        const sms = self.shader_modules[0..self.stage_count];
        for (sms) |sm| VKShaderModule.get(sm).recreate();
        if (!for (sms) |sm| {
            if (VKShaderModule.get(sm).load_state == .fail)
                break false;
        } else true) return;
        const new = create(sms, self.params) catch return;
        device.destroyDescriptorSetLayout(self.descriptor_set_layout, vka);
        device.freeDescriptorSets(descriptor_pools[0], 1, &.{self.descriptor_set}) catch {};

        device.destroyPipelineLayout(self.layout, vka);
        device.destroyPipeline(self.vk_handle, vka);
        self.* = new;
    }
    pub fn create(stage_modules: []const ShaderModule, params: gpu.PipelineParams) Error!VKPipeline {
        var pipeline: VKPipeline = .{ .params = params };
        @memcpy(pipeline.shader_modules[0..stage_modules.len], stage_modules);

        const mps = gpu.Pipeline.max_pipeline_stages;
        var stage_create_infos: [mps]vk.PipelineShaderStageCreateInfo = undefined;

        var bindings: List(vk.DescriptorSetLayoutBinding) = .empty;
        defer bindings.deinit(arena.allocator());

        for (0..stage_modules.len) |i| {
            const shader_module: *VKShaderModule = .get(stage_modules[i]);
            const ep_info = shader_module.entry_point_info;
            if (i != 0) {
                const previous_stage_ep_info = VKShaderModule.get(stage_modules[i - 1]).entry_point_info;
                for (ep_info.inputMappings()) |im| {
                    const @"type" = for (previous_stage_ep_info.outputMappings()) |om| {
                        if (om.location == im.location) break om.type;
                    } else return Error.PipelineStageIOMismatch;
                    if (!im.type.eql(@"type")) return Error.PipelineStageIOMismatch;
                }
            }
            for (ep_info.opaque_uniform_mappings) |oum| {
                const descriptor_type: vk.DescriptorType = getDescriptorTypeHGSL(oum.type);
                for (bindings.items) |*b| {
                    if (b.binding == oum.binding) {
                        if (b.descriptor_type != descriptor_type)
                            return Error.PipelineDescriptorCollision;
                        b.stage_flags = b.stage_flags.merge(getStageFlags(ep_info.stage_info));
                        break;
                    }
                } else try bindings.append(arena.allocator(), .{
                    .binding = oum.binding,
                    .descriptor_type = descriptor_type,
                    .descriptor_count = 1,
                    .stage_flags = getStageFlags(ep_info.stage_info),
                });
            }

            const stage_flags = getStageFlags(ep_info.stage_info);
            stage_create_infos[i] = .{
                .module = shader_module.vk_handle,
                .p_name = ep_info.name,
                .stage = stage_flags,
            };
            pipeline.entry_point_info_storage[i] = ep_info;
            pipeline.stage_count = @intCast(stage_modules.len);

            if (ep_info.push_constant_mappings.len != 0) {
                const last_pc = ep_info.push_constant_mappings[ep_info.push_constant_mappings.len - 1];
                pipeline.push_constant_ranges[pipeline.push_constant_range_count] = .{
                    .stage_flags = stage_flags,
                    .offset = 0,
                    .size = last_pc.offset + last_pc.size,
                };
                pipeline.push_constant_range_count += 1;
            }
        }
        pipeline.sortRanges();
        //CHECK stages *compatibility

        const fragment: ?FragmentStageInfo = FragmentStageInfo.fromStageSlice(stage_modules);
        const vertex: ?VertexStageInfo = VertexStageInfo.fromStageSlice(stage_modules);

        pipeline.descriptor_set_layout = device.createDescriptorSetLayout(&.{
            .binding_count = @intCast(bindings.items.len),
            .p_bindings = bindings.items.ptr,
        }, vka) catch return Error.ResourceCreationError;
        device.allocateDescriptorSets(&.{
            .descriptor_pool = descriptor_pools[0],
            .descriptor_set_count = 1,
            .p_set_layouts = &.{pipeline.descriptor_set_layout},
        }, @ptrCast(&pipeline.descriptor_set)) catch
            return Error.ResourceCreationError;

        const layout = device.createPipelineLayout(&.{
            .set_layout_count = 1,
            .p_set_layouts = &.{pipeline.descriptor_set_layout},
            .push_constant_range_count = pipeline.push_constant_range_count,
            .p_push_constant_ranges = &pipeline.push_constant_ranges,
        }, vka) catch return Error.ResourceCreationError;
        pipeline.layout = layout;

        _ = device.createGraphicsPipelines(.null_handle, 1, &.{.{
            .p_next = if (fragment) |frag| &(try frag.pipelineCreationInfo(
                &.{.b8g8r8a8_unorm},
                .undefined,
                .undefined,
            )) else null,
            .stage_count = @intCast(stage_modules.len),
            .p_stages = stage_create_infos[0..stage_modules.len].ptr,
            .layout = layout,

            .p_vertex_input_state = if (vertex) |vert| &.{
                .vertex_binding_description_count = 1,
                .p_vertex_binding_descriptions = &.{vert.binding_description},
                .vertex_attribute_description_count = @intCast(vert.attributes.len),
                .p_vertex_attribute_descriptions = vert.attributes.ptr,
            } else null,
            .p_input_assembly_state = if (vertex) |_| &.{
                .topology = castPrimitiveTopology(params.primitive),
                .primitive_restart_enable = .false,
            } else null,
            // p_tessellation_state: ?*const PipelineTessellationStateCreateInfo = null,
            .p_viewport_state = if (fragment) |frag| &.{
                .viewport_count = frag.output_count,
                .scissor_count = frag.output_count,
            } else null,
            // p_rasterization_state: ?*const PipelineRasterizationStateCreateInfo = null,
            .p_rasterization_state = &.{
                .depth_clamp_enable = .false,
                .rasterizer_discard_enable = .false,
                .polygon_mode = .fill,
                .line_width = 1,
                .cull_mode = .{
                    .back_bit = params.cull == .back or params.cull == .both,
                    .front_bit = params.cull == .front or params.cull == .both,
                },
                .front_face = if (params.winding_order == .clockwise) .clockwise else .counter_clockwise,
                .depth_bias_enable = .false,
                .depth_bias_constant_factor = 0,
                .depth_bias_clamp = 0,
                .depth_bias_slope_factor = 0,
            },

            .p_multisample_state = &.{
                .sample_shading_enable = .false,
                .rasterization_samples = .{ .@"1_bit" = true },
                .min_sample_shading = 1,
                .p_sample_mask = null,
                .alpha_to_coverage_enable = .false,
                .alpha_to_one_enable = .false,
            },
            // p_depth_stencil_state: ?*const PipelineDepthStencilStateCreateInfo = null,
            .p_color_blend_state = if (fragment) |frag| &.{
                .logic_op_enable = .false,
                .logic_op = .clear,
                .attachment_count = frag.output_count,
                .p_attachments = &.{.{
                    .color_write_mask = .{
                        .r_bit = true,
                        .b_bit = true,
                        .g_bit = true,
                        .a_bit = true,
                    },
                    .blend_enable = .false,
                    .src_color_blend_factor = .one,
                    .dst_color_blend_factor = .zero,
                    .color_blend_op = .add,
                    .src_alpha_blend_factor = .one,
                    .dst_alpha_blend_factor = .zero,
                    .alpha_blend_op = .add,
                }},
                .blend_constants = @splat(0),
            } else null,
            .p_dynamic_state = &.{
                .p_dynamic_states = &.{ .viewport, .scissor },
                .dynamic_state_count = 2,
            },

            .subpass = 0,
            .base_pipeline_index = -1,
            .flags = .{},
        }}, vka, @ptrCast(&pipeline.vk_handle)) catch
            return Error.ResourceCreationError;
        return pipeline;
    }
    pub fn destroy(self: *VKPipeline) void {
        device.destroyPipelineLayout(self.layout, vka);
        device.destroyPipeline(self.vk_handle, vka);
        self.* = .{};
    }
    fn sortRanges(self: *VKPipeline) void {
        if (self.push_constant_range_count < 2) return;
        for (0..self.push_constant_range_count - 1) |i| {
            for (i + 1..self.push_constant_range_count) |j| {
                if (self.push_constant_ranges[i].size < self.push_constant_ranges[j].size)
                    std.mem.swap(vk.PushConstantRange, &self.push_constant_ranges[i], &self.push_constant_ranges[j]);
            }
        }
    }
    pub fn get(handle: Pipeline) *VKPipeline {
        return &pipeline_list.items[@intFromEnum(handle)];
    }
};
const FragmentStageInfo = struct {
    output_count: u32,

    pub fn fromStageSlice(stages: []const ShaderModule) ?FragmentStageInfo {
        const ep_info: hgsl.EntryPointInfo = for (stages) |s| {
            const shader_module: *VKShaderModule = VKShaderModule.get(s);
            if (shader_module.entry_point_info.stage_info == .fragment)
                break shader_module.entry_point_info;
        } else return null;
        _ = ep_info;
        return .{
            .output_count = 1, //TODO: check that
        };
    }
    pub fn pipelineCreationInfo(
        self: *const FragmentStageInfo,
        formats: []const vk.Format,
        depth_format: vk.Format,
        stencil_format: vk.Format,
    ) Error!vk.PipelineRenderingCreateInfo {
        //
        if (formats.len != self.output_count) return Error.ResourceCreationError;
        return .{
            .color_attachment_count = self.output_count,
            .p_color_attachment_formats = formats.ptr,
            .depth_attachment_format = depth_format,
            .stencil_attachment_format = stencil_format,
            .view_mask = 0,
        };
    }
};
const VertexStageInfo = struct {
    attributes: []const vk.VertexInputAttributeDescription = &.{},
    binding_description: vk.VertexInputBindingDescription,
    var attribute_storage: [10]vk.VertexInputAttributeDescription = undefined;

    pub fn fromStageSlice(stages: []const ShaderModule) ?VertexStageInfo {
        const ep_info: hgsl.EntryPointInfo = for (stages) |s| {
            const shader_module: *VKShaderModule = .get(s);
            if (shader_module.entry_point_info.stage_info == .vertex)
                break shader_module.entry_point_info;
        } else return null;
        if (ep_info.inputMappings().len > attribute_storage.len)
            @panic("TODO: unlimited vertex attributes");

        var stride: u32 = 0;
        for (ep_info.inputMappings(), 0..) |im, i| {
            attribute_storage[i] = .{
                .location = im.location,
                .binding = 0,
                .format = formatFromIOType(im.type),
                .offset = stride,
            };
            stride += im.size;
        }
        return .{
            .attributes = attribute_storage[0..ep_info.input_count],
            .binding_description = .{
                .binding = 0,
                .stride = stride,
                .input_rate = .vertex,
            },
        };
    }
};
pub const VKBuffer = struct {
    vk_handle: vk.Buffer = .null_handle,
    device_memory: u32 = u32m,
    size: u64 = 0,
    usage: BufferUsage = undefined,

    mapped: bool = false,
    pub fn create(size: u64, usage: BufferUsage) Error!VKBuffer {
        if (size == 0) return .{ .size = 0, .usage = usage };

        const handle = device.createBuffer(&.{
            .size = size,
            .usage = .{
                .vertex_buffer_bit = usage == .vertex,
                .index_buffer_bit = usage == .index,
                .uniform_buffer_bit = usage == .uniform,
                .transfer_src_bit = true,
                .transfer_dst_bit = true,
            },
            .sharing_mode = .exclusive,
            // .p_queue_family_indices = &.{},
            // .queue_family_index_count = 1,
        }, vka) catch return Error.ResourceCreationError;

        const memory_requirements = device.getBufferMemoryRequirements(handle);
        const memory = try allocateDeviceMemory(memory_requirements, .{ .host_visible_bit = true });

        device.bindBufferMemory(handle, getMemoryReference(memory), 0) catch
            return Error.ResourceCreationError;
        return .{
            .vk_handle = handle,
            .device_memory = memory,
            .size = size,
            .usage = usage,
        };
    }
    pub fn destroy(self: *VKBuffer) void {
        device.destroyBuffer(self.vk_handle, vka);
        removeMemoryReference(self.device_memory);
        self.* = .{};
    }
    pub fn get(handle: Buffer) *VKBuffer {
        return &buffer_list.items[@intFromEnum(handle)];
    }
    pub fn append(buffer: VKBuffer) Error!Buffer {
        const handle: Buffer = @enumFromInt(@as(gpu.Handle, @intCast(buffer_list.items.len)));
        try buffer_list.append(arena.allocator(), buffer);
        return handle;
    }
};

fn accessMemory(index: u32) vk.DeviceMemory {
    return memory_allocation_list.items[index].memory;
}
pub fn getMemoryReference(index: u32) vk.DeviceMemory {
    const ptr = &memory_allocation_list.items[index];
    ptr.ref_count += 1;
    return ptr.memory;
}
pub fn removeMemoryReference(index: u32) void {
    const ptr = &memory_allocation_list.items[index];
    ptr.ref_count -|= 1;
    if (ptr.ref_count == 0) ptr.free();
}
pub fn allocateDeviceMemory(memory_requirements: vk.MemoryRequirements, flags: vk.MemoryPropertyFlags) Error!u32 {
    std.debug.print("ALLOC: {d} KiB\n", .{@as(f64, @floatFromInt(memory_requirements.size)) / 1024.0});
    const mem_type: u32 = for (0..physical_device_memory_properties.memory_type_count) |i| {
        if ((memory_requirements.memory_type_bits & (@as(u32, 1) << @as(u5, @intCast(i)))) == 0) continue;
        if (physical_device_memory_properties.memory_types[i].property_flags.contains(flags))
            break @intCast(i);
    } else return Error.ResourceCreationError;

    const device_memory = device.allocateMemory(&.{
        .allocation_size = memory_requirements.size,
        .memory_type_index = mem_type,
    }, vka) catch
        return Error.ResourceCreationError;
    const index: u32 = @intCast(memory_allocation_list.items.len);
    try memory_allocation_list.append(arena.allocator(), .{ .memory = device_memory });
    return index;
}

const VKRenderTarget = struct {
    color_attachment: Texture = @enumFromInt(0),
    depth_stencil_attachment: Texture = @enumFromInt(0),
    mask: Mask = .{},

    pub fn size(self: VKRenderTarget) math.uvec2 {
        const ct: *VKTexture = if (self.mask.color)
            .get(self.color_attachment)
        else
            .get(self.depth_stencil_attachment);
        const tex_size = ct.dimensions.size();
        return .{ tex_size[0], tex_size[1] };
    }

    pub fn create(
        tex_size: math.uvec2,
        color_format: ?Format,
        depth_stencil_format: ?Format,
        sampling_options: ?gpu.SamplingOptions,
    ) Error!VKRenderTarget {
        if (color_format == null and depth_stencil_format == null)
            @panic("both formats are null(should be checked before calling)");
        var result: VKRenderTarget = .{ .mask = .{
            .color = color_format != null,
            .depth_stencil = depth_stencil_format != null,
        } };
        var tex_create_params: [2]VKTexture.TextureCreateParams = @splat(.{
            .dimensions = .{ .@"2d" = tex_size },
            .is_attachment = true,
            .sampling_options = sampling_options,
        });
        var i: usize = 0;
        if (color_format) |f| tex_create_params[util.ipp(&i)].format = f;
        if (depth_stencil_format) |f| tex_create_params[util.ipp(&i)].format = f;
        var textures: [2]VKTexture = undefined;
        try VKTexture.createSlice(textures[0..i], tex_create_params[0..i]);

        if (color_format) |_| result.color_attachment = try textures[0].append();
        if (depth_stencil_format) |_| result.depth_stencil_attachment = try textures[i - 1].append();
        return result;
    }
    pub fn destroy(self: *VKRenderTarget) void {
        self.mask = .{};
    }
    pub fn append(self: VKRenderTarget) Error!RenderTarget {
        const handle: RenderTarget = @enumFromInt(@as(gpu.Handle, @intCast(render_target_list.items.len)));
        try render_target_list.append(arena.allocator(), self);
        return handle;
    }
    pub fn get(handle: RenderTarget) *VKRenderTarget {
        return &render_target_list.items[~@as(gpu.Handle, 1 << 31) & @intFromEnum(handle)];
    }

    const Mask = packed struct { color: bool = false, depth_stencil: bool = false };
};
fn getDrawCommandCmd(queue_type: QueueType) vk.CommandBuffer {
    huge.dassert(current_render_target != null_render_target);
    if (current_render_target == null_render_target) return .null_handle;
    return if (VKWindowContext.fromRenderTarget(current_render_target)) |wc|
        wc.beginCmd(queue_type) catch .null_handle
    else
        @panic("init rendering resources NOT of WINDOW rt");
}

const VKWindowContext = struct {
    const mic = 3; //max_image_count
    const mfif = mic - 1; //max_frame_in_flight
    acquired_image_index: u32 = u32m,

    fif_index: u32 = 0, //current frame-in-flight index

    surface: vk.SurfaceKHR = .null_handle,
    request_recreate: bool = false,
    swapchain: vk.SwapchainKHR = .null_handle,

    images: [mic]vk.Image = @splat(.null_handle),
    image_views: [mic]vk.ImageView = @splat(.null_handle),
    image_count: u32 = 0,

    extent: vk.Extent2D = .{ .width = 0, .height = 0 },
    surface_format: vk.SurfaceFormatKHR = undefined,
    present_mode: vk.PresentModeKHR = .fifo_khr,

    current_frame: usize = 0,
    acquire_semaphores: [mfif]vk.Semaphore = @splat(.null_handle),
    submit_semaphores: [mic]vk.Semaphore = @splat(.null_handle),
    fences: [mfif]vk.Fence = @splat(.null_handle),

    //render target
    cmds: [mfif][queue_type_count]vk.CommandBuffer = @splat(@splat(.null_handle)),
    cmd_mask: u64 = 0,

    inline fn fif(self: VKWindowContext) u32 {
        return @max(@max(self.image_count, 1) - 1, 1);
    }
    pub fn beginRenderingToWindow(self: *VKWindowContext, clear_value: ClearValue) Error!void {
        //INCREMENT FIF INDEX
        // self.fif_index = (self.fif_index + 1) % self.fif();

        if (~self.acquired_image_index != 0) return;

        _ = device.waitForFences(1, &.{self.fences[self.fif_index]}, .true, timeout) catch
            return Error.SynchronisationError;
        device.resetFences(1, &.{self.fences[self.fif_index]}) catch
            return Error.SynchronisationError;
        self.acquired_image_index = (device.acquireNextImageKHR(
            self.swapchain,
            timeout,
            self.acquire_semaphores[self.fif_index],
            .null_handle,
        ) catch return Error.ResourceCreationError).image_index;

        //begin on demand
        const cmd = try self.beginCmd(.graphics);

        const rt_size = renderTargetSize(current_render_target);
        device.cmdSetScissor(cmd, 0, 1, &.{.{
            .extent = .{ .width = rt_size[0], .height = rt_size[1] },
            .offset = .{ .x = 0, .y = 0 },
        }});

        device.cmdSetViewport(cmd, 0, 1, &.{.{
            .x = 0,
            .y = 0,
            .width = @floatFromInt(rt_size[0]),
            .height = @floatFromInt(rt_size[1]),
            .min_depth = 0,
            .max_depth = 1,
        }});
        device.cmdBeginRendering(cmd, &.{
            .render_area = .{
                .offset = .{ .x = 0, .y = 0 },
                .extent = self.extent,
            },
            .color_attachment_count = 1,
            .p_color_attachments = &.{.{
                .image_view = self.image_views[self.acquired_image_index],
                .image_layout = .attachment_optimal,
                .load_op = .clear,
                .store_op = .store,
                .clear_value = .{
                    .color = if (clear_value.color) |cc|
                        .{ .float_32 = @as(*const [4]f32, @ptrCast(&cc)).* }
                    else
                        .{ .float_32 = @splat(0) },
                },

                .resolve_image_layout = .undefined,
                .resolve_mode = .{},
            }},

            .flags = .{},
            .layer_count = 1,
            .view_mask = 0,
            // p_depth_attachment: ?*const RenderingAttachmentInfo = null,
            // p_stencil_attachment: ?*const RenderingAttachmentInfo = null,
        });
    }

    pub fn endRenderingToWindow(self: *VKWindowContext) Error!void {
        defer self.acquired_image_index = u32m;

        device.cmdEndRendering(self.getCmd(.graphics));
        for (self.cmds[self.fif_index], 0..) |cmd, i| {
            if (cmd != .null_handle and
                self.getBit(@enumFromInt(i), self.cmd_mask) and
                pd().getUniqueQFIIndex(@enumFromInt(i)) != pd().getUniqueQFIIndex(.presentation))
            {
                device.endCommandBuffer(cmd) catch
                    return Error.PresentationError;
                self.flipBit(@enumFromInt(i), &self.cmd_mask);
            }
        } //submit all queues that were recorded
        self.present(self.getCmd(.presentation)) catch
            return Error.PresentationError;
        self.flipBit(.presentation, &self.cmd_mask);
    }
    pub fn forceEndRendering(self: *VKWindowContext) void {
        defer self.acquired_image_index = u32m;

        device.cmdEndRendering(self.getCmd(.graphics));
        for (self.cmds[self.fif_index], 0..) |cmd, i| {
            if (cmd != .null_handle and i != pd().getUniqueQFIIndex(.presentation))
                device.endCommandBuffer(cmd) catch {};
        } //submit all queues that were recorded
        device.endCommandBuffer(self.getCmd(.presentation)) catch {};
        device.queueSubmit(queue(.presentation), 1, &.{.{
            .command_buffer_count = 0,
            .p_wait_dst_stage_mask = &.{.{ .color_attachment_output_bit = true }},
            .wait_semaphore_count = 1,
            .p_wait_semaphores = &.{self.acquire_semaphores[self.fif_index]},
        }}, self.fences[self.fif_index]) catch {};
    }
    fn beginCmd(self: *VKWindowContext, queue_type: QueueType) Error!vk.CommandBuffer {
        const cmd = self.getCmd(queue_type);
        if (!self.getBit(queue_type, self.cmd_mask) and cmd != .null_handle) {
            device.resetCommandBuffer(cmd, .{}) catch
                return Error.ResourceCreationError;
            device.beginCommandBuffer(cmd, &.{}) catch
                return Error.ResourceCreationError;
            self.flipBit(queue_type, &self.cmd_mask);
        }
        return cmd;
    }
    fn getCmd(self: *VKWindowContext, queue_type: QueueType) vk.CommandBuffer {
        return self.cmds[self.fif_index][pd().getUniqueQFIIndex(queue_type)];
    }
    fn getBit(self: *VKWindowContext, queue_type: QueueType, mask: u64) bool {
        const bit: u6 = @truncate(queue_type_count * self.fif_index + pd().getUniqueQFIIndex(queue_type));
        return ((mask) >> bit) & 1 > 0;
    }
    fn flipBit(self: *VKWindowContext, queue_type: QueueType, mask: *u64) void {
        const bit: u6 = @truncate(queue_type_count * self.fif_index + pd().getUniqueQFIIndex(queue_type));
        mask.* ^= @as(u64, 1) << bit;
    }

    fn present(self: VKWindowContext, cmd: vk.CommandBuffer) !void {
        const image_barrier: vk.ImageMemoryBarrier = .{
            .src_access_mask = .{ .color_attachment_write_bit = true },
            .dst_access_mask = .{},
            .old_layout = .undefined,
            .new_layout = .present_src_khr,
            .src_queue_family_index = pd().queueFamilyIndex(.presentation),
            .dst_queue_family_index = pd().queueFamilyIndex(.presentation),
            .image = self.images[self.acquired_image_index],
            .subresource_range = .{
                .aspect_mask = .{ .color_bit = true },
                .base_mip_level = 0,
                .level_count = 1,
                .base_array_layer = 0,
                .layer_count = 1,
            },
        };
        device.cmdPipelineBarrier(cmd, .{
            .all_commands_bit = true,
        }, .{
            .bottom_of_pipe_bit = true,
        }, .{}, 0, null, 0, null, 1, &.{
            image_barrier,
        });

        try device.endCommandBuffer(cmd);
        try device.queueSubmit(
            queue(.presentation),
            1,
            &.{.{
                .command_buffer_count = 1,
                .p_command_buffers = &.{cmd},
                .p_wait_dst_stage_mask = &.{.{ .color_attachment_output_bit = true }},

                .wait_semaphore_count = 1,
                .p_wait_semaphores = &.{self.acquire_semaphores[self.fif_index]},
                .signal_semaphore_count = 1,
                .p_signal_semaphores = &.{self.submit_semaphores[self.acquired_image_index]},
            }},
            self.fences[self.fif_index],
        );

        _ = device.queuePresentKHR(queue(.presentation), &.{
            .wait_semaphore_count = 1,
            .p_wait_semaphores = &.{self.submit_semaphores[self.acquired_image_index]},
            .swapchain_count = 1,
            .p_swapchains = &.{self.swapchain},
            .p_image_indices = &.{self.acquired_image_index},
        }) catch |err|
            switch (err) {
                error.OutOfDateKHR => {
                    @panic("recreate swapchain");
                    // self.request_recreate = true;
                },
                else => return error.PresentationError,
            };
    }
    fn fromRenderTarget(render_target: RenderTarget) ?*VKWindowContext {
        const int: gpu.Handle = @intFromEnum(render_target);
        if ((int >> 31) == 0) return null;
        return VKWindowContext.get(@enumFromInt(int & ~@as(gpu.Handle, 1 << 31)));
    }

    pub fn create(window: huge.Window) !VKWindowContext {
        var surface_handle: u64 = undefined;
        if (glfw.createWindowSurface(
            @intFromEnum(instance.handle),
            window.handle,
            null,
            &surface_handle,
        ) != .success) return Error.WindowContextCreationError;
        var result: VKWindowContext = .{
            .surface = @enumFromInt(surface_handle),
        };

        const capabilities = try instance.getPhysicalDeviceSurfaceCapabilitiesKHR(pd().handle, result.surface);

        result.extent = blk: {
            if (capabilities.current_extent.width == std.math.maxInt(u32)) {
                var res: [2]c_int = @splat(0);
                glfw.getFramebufferSize(window.handle, &res[0], &res[1]);
                break :blk .{
                    .width = std.math.clamp(@as(u32, @intCast(res[0])), capabilities.min_image_extent.width, capabilities.max_image_extent.width),
                    .height = std.math.clamp(@as(u32, @intCast(res[1])), capabilities.min_image_extent.height, capabilities.max_image_extent.height),
                };
            }
            break :blk .{
                .width = std.math.clamp(capabilities.current_extent.width, capabilities.min_image_extent.width, capabilities.max_image_extent.width),
                .height = std.math.clamp(capabilities.current_extent.height, capabilities.min_image_extent.height, capabilities.max_image_extent.height),
            };
        };

        result.surface_format = blk: {
            const max_surface_format_count = 100;
            var surface_format_count: u32 = 0;
            _ = try instance.getPhysicalDeviceSurfaceFormatsKHR(pd().handle, result.surface, &surface_format_count, null);
            surface_format_count = @min(max_surface_format_count, surface_format_count);
            var surface_format_storage: [max_surface_format_count]vk.SurfaceFormatKHR = undefined;
            _ = try instance.getPhysicalDeviceSurfaceFormatsKHR(pd().handle, result.surface, &surface_format_count, &surface_format_storage);
            break :blk for (surface_format_storage[0..surface_format_count]) |sf| {
                if (sf.format == .b8g8r8a8_unorm and sf.color_space == .srgb_nonlinear_khr) break sf;
            } else surface_format_storage[0];
        };

        result.present_mode = blk: {
            var present_mode_storage: [@typeInfo(vk.PresentModeKHR).@"enum".fields.len]vk.PresentModeKHR = undefined;
            var present_mode_count: u32 = 0;
            _ = try instance.getPhysicalDeviceSurfacePresentModesKHR(pd().handle, result.surface, &present_mode_count, null);
            _ = try instance.getPhysicalDeviceSurfacePresentModesKHR(pd().handle, result.surface, &present_mode_count, &present_mode_storage);
            break :blk for (present_mode_storage[0..present_mode_count]) |pm| {
                if (pm == vk.PresentModeKHR.mailbox_khr) break pm;
            } else vk.PresentModeKHR.fifo_khr;
        };

        result.image_count = @max(capabilities.min_image_count, @as(u32, if (result.present_mode == .mailbox_khr) 3 else 2));
        const exclusive = pd().queueFamilyIndex(.graphics) == pd().queueFamilyIndex(.presentation);
        result.swapchain = try device.createSwapchainKHR(&.{
            .surface = result.surface,
            .min_image_count = result.image_count,

            .present_mode = result.present_mode,
            .image_format = result.surface_format.format,
            .image_color_space = result.surface_format.color_space,
            .image_extent = result.extent,

            .image_array_layers = 1,
            .image_sharing_mode = if (exclusive) .exclusive else .concurrent,
            .image_usage = .{
                .transfer_dst_bit = true,
                .color_attachment_bit = true,
            },
            .queue_family_index_count = if (exclusive) 0 else 2,
            .p_queue_family_indices = if (exclusive) null else &.{
                pd().queueFamilyIndex(.graphics),
                pd().queueFamilyIndex(.presentation),
            },
            .pre_transform = capabilities.current_transform,
            .composite_alpha = .{ .opaque_bit_khr = true },
            .clipped = .true,
        }, vka);

        _ = try device.getSwapchainImagesKHR(result.swapchain, &result.image_count, null);
        result.image_count = @min(result.image_count, mic);
        _ = try device.getSwapchainImagesKHR(result.swapchain, &result.image_count, &result.images);

        for (0..result.image_count) |i|
            result.image_views[i] = try device.createImageView(&.{
                .components = .{ .r = .identity, .g = .identity, .b = .identity, .a = .identity },
                .format = result.surface_format.format,
                .image = result.images[i],
                .subresource_range = .{
                    .aspect_mask = .{ .color_bit = true },
                    .layer_count = 1,
                    .base_array_layer = 0,
                    .level_count = 1,
                    .base_mip_level = 0,
                },
                .view_type = .@"2d",
            }, vka);

        for (0..result.fif()) |i| {
            result.acquire_semaphores[i] = try device.createSemaphore(&.{}, vka);
            result.fences[i] = try device.createFence(&.{ .flags = .{ .signaled_bit = true } }, vka);
            for (0..queue_type_count) |j| {
                if (~pd().queue_family_indices[j] == 0) continue;
                const index = pd().getUniqueQFIIndex(@enumFromInt(j));
                if (result.cmds[i][index] == .null_handle)
                    result.cmds[i][index] = try allocateCommandBuffer(@enumFromInt(i), .primary);
            }
        }
        for (0..result.image_count) |i|
            result.submit_semaphores[i] = try device.createSemaphore(&.{}, vka);

        return result;
    }
    pub fn destroy(self: *VKWindowContext, handle: ?WindowContext) void {
        if (handle != null and current_render_target == getWindowContextRenderTarget(handle.?)) {
            self.forceEndRendering();
        }
        device.queueWaitIdle(queue(.presentation)) catch {};

        for (self.image_views[0..self.image_count]) |iw|
            device.destroyImageView(iw, vka);
        device.destroySwapchainKHR(self.swapchain, vka);

        for (0..self.fif()) |i| {
            device.destroySemaphore(self.acquire_semaphores[i], vka);
            device.destroyFence(self.fences[i], vka);
        }
        for (0..self.image_count) |i|
            device.destroySemaphore(self.submit_semaphores[i], vka);

        instance.destroySurfaceKHR(self.surface, vka);
        self.* = .{};
    }
    pub fn get(handle: WindowContext) *VKWindowContext {
        return if (@intFromEnum(handle) == 0)
            &window_context_primary
        else
            @panic("");
    }
};

fn commandPool(queue_type: QueueType) Error!vk.CommandPool {
    const index = @intFromEnum(queue_type);
    if (command_pools[index] == .null_handle) {
        const qfi = pd().queueFamilyIndex(queue_type);
        const created = device.createCommandPool(&.{
            .flags = .{ .reset_command_buffer_bit = true },
            .queue_family_index = qfi,
        }, vka) catch
            return Error.ResourceCreationError;
        for (&command_pools, 0..) |*cmd_pools, i| {
            if (pd().queueFamilyIndex(@enumFromInt(i)) == qfi)
                cmd_pools.* = created;
        }
    }
    return command_pools[index];
}
fn allocateCommandBuffer(queue_type: QueueType, level: vk.CommandBufferLevel) Error!vk.CommandBuffer {
    var result: vk.CommandBuffer = .null_handle;
    device.allocateCommandBuffers(&.{
        .command_pool = try commandPool(queue_type),
        .level = level,
        .command_buffer_count = 1,
    }, @ptrCast(&result)) catch return Error.ResourceCreationError;
    return result;
}
//===|vkextensions|====

fn isDynamicRenderingBuiltin() bool {
    return api_version.@">="(.{ .major = 1, .minor = 3 });
}
fn cmdBeginRendering(command_buffer: vk.CommandBuffer, rendering_info: *const vk.RenderingInfo) void {
    if (isDynamicRenderingBuiltin())
        device.cmdBeginRendering(command_buffer, rendering_info)
    else
        device.cmdBeginRenderingKHR(command_buffer, rendering_info);
}
fn cmdEndRendering(command_buffer: vk.CommandBuffer, rendering_info: *const vk.RenderingInfo) void {
    if (isDynamicRenderingBuiltin())
        device.cmdEndRendering(command_buffer, rendering_info)
    else
        device.cmdEndRenderingKHR(command_buffer, rendering_info);
}

//===|initialization|====

pub fn initBackend() VKError!gpu.Backend {
    bwp = .load(loader);
    arena = .init(std.heap.page_allocator);

    const instance_api_version = castVersion(@bitCast(bwp.enumerateInstanceVersion() catch return error.OutOfMemory));
    if (!instance_api_version.@">="(min_vulkan_version))
        return VKError.UnsupportedApiVersion;
    try initInstance(arena.allocator(), instance_api_version);

    var extension_name_buf: [10][*:0]const u8 = undefined;
    var device_extension_list: List([*:0]const u8) = .initBuffer(&extension_name_buf);

    device_extension_list.appendAssumeCapacity(vk.extensions.khr_swapchain.name);
    if (!instance_api_version.@">="(.{ .major = 1, .minor = 3 }))
        device_extension_list.appendAssumeCapacity(vk.extensions.khr_dynamic_rendering.name);

    try initPhysicalDevices(arena.allocator(), device_extension_list.items);
    const physical_device_api_version = castVersion(@bitCast(instance.getPhysicalDeviceProperties(pd().handle).api_version));

    physical_device_memory_properties = instance.getPhysicalDeviceMemoryProperties(pd().handle);
    api_version = if (physical_device_api_version.@">="(instance_api_version)) instance_api_version else physical_device_api_version;
    try initLogicalDeviceAndQueues(device_extension_list.items);

    descriptor_pools[0] = device.createDescriptorPool(&.{
        .flags = .{
            .free_descriptor_set_bit = huge.zigbuiltin.mode == .Debug,
        },
        .max_sets = 100,
        .pool_size_count = 1,
        .p_pool_sizes = &.{.{
            .type = .uniform_buffer,
            .descriptor_count = 10,
        }},
    }, vka) catch return VKError.ResourceCreationError;

    shader_compiler = .new(null, null, .{
        .target_env = .vulkan1_4,
        .max_push_constant_buffer_size = max_push_constant_bytes,
    });
    return versionBackend(api_version);
}

pub fn deinit() void {
    defer if (arena.state.end_index != 0) {
        _ = arena.deinit();
        arena.state.end_index = 0;
    };
    endRendering() catch {};

    device.deviceWaitIdle() catch {};
    shader_compiler.deinit();

    const let_os_deinit = false;
    if (let_os_deinit) return;

    for (&command_pools) |cmd_pool| {
        for (&command_pools) |*i| {
            if (cmd_pool == i.*) i.* = .null_handle;
        }
        device.destroyCommandPool(cmd_pool, vka);
    }
    for (shader_module_list.items) |*i| i.destroy();
    shader_module_list.items = &.{};
    for (pipeline_list.items) |*i| i.destroy();
    pipeline_list.items = &.{};
    for (buffer_list.items) |*i| i.destroy();
    buffer_list.items = &.{};
    for (texture_list.items) |*i| i.destroy();
    texture_list.items = &.{};
    for (memory_allocation_list.items) |*i| i.free();
    memory_allocation_list.items = &.{};
    for (render_target_list.items) |*i| i.destroy();
    render_target_list.items = &.{};
    for (window_context_list.items) |*i| i.destroy(null);
    window_context_list.items = &.{};
    window_context_primary.destroy(null);

    for (&descriptor_pools) |dp| if (dp != .null_handle)
        device.destroyDescriptorPool(dp, vka);

    // device.destroyDevice(vka);
    // instance.destroyInstance(vka);
}
fn initLogicalDeviceAndQueues(extensions: []const [*:0]const u8) VKError!void {
    var queue_create_infos: [queue_type_count]vk.DeviceQueueCreateInfo = undefined;
    var queues_to_create: [queue_type_count]u8 = undefined;
    var count: usize = 0;

    //track all unique queue families
    for (pd().queue_family_indices) |qi| {
        if (~qi == 0) continue;

        for (0..count) |c| {
            if (queues_to_create[c] == qi) break;
        } else {
            queues_to_create[count] = qi;
            queue_create_infos[count] = .{
                .queue_count = 1,
                .queue_family_index = qi,
                .p_queue_priorities = &.{1.0},
            };
            count += 1;
        }
    }

    const dynamic_rendering_feature_ptr: *const anyopaque =
        if (isDynamicRenderingBuiltin())
            &vk.PhysicalDeviceDynamicRenderingFeatures{ .dynamic_rendering = .true }
        else
            &vk.PhysicalDeviceDynamicRenderingFeaturesKHR{ .dynamic_rendering = .true };

    const device_create_info: vk.DeviceCreateInfo = .{
        .enabled_extension_count = @intCast(extensions.len),
        .pp_enabled_extension_names = extensions.ptr,
        .queue_create_info_count = @intCast(count),
        .p_queue_create_infos = &queue_create_infos,
        .pp_enabled_layer_names = layers.ptr,
        .enabled_layer_count = @intCast(layers.len),

        .p_next = dynamic_rendering_feature_ptr,
    };

    const device_handle = instance.createDevice(
        pd().handle,
        &device_create_info,
        vka,
    ) catch return VKError.LogicalDeviceInitializationFailure;
    dwp = .load(device_handle, instance.wrapper.dispatch.vkGetDeviceProcAddr.?);
    device = .init(device_handle, &dwp);

    for (0..queue_type_count) |i| {
        if (~pd().queue_family_indices[i] != 0)
            queues[i] = device.getDeviceQueue(pd().queue_family_indices[i], 0);
    }
}
fn initInstance(allocator: Allocator, instance_api_version: Version) VKError!void {
    const available_layers: []vk.LayerProperties =
        if (layers.len > 0) bwp.enumerateInstanceLayerPropertiesAlloc(allocator) catch return VKError.OutOfMemory else &.{};

    try checkLayerPresence(layers, available_layers);
    if (layers.len > 0) allocator.free(available_layers);

    var glfw_ext_count: u32 = 0; //get platform presentation extensions
    const glfw_exts = glfw.getRequiredInstanceExtensions(&glfw_ext_count);
    const instance_extensions: []const [*:0]const u8 = if (glfw_exts) |ge| ge[0..glfw_ext_count] else &.{};

    const available_instance_extensions: []vk.ExtensionProperties =
        bwp.enumerateInstanceExtensionPropertiesAlloc(null, allocator) catch return VKError.OutOfMemory;

    try checkExtensionPresence(instance_extensions, available_instance_extensions);
    allocator.free(available_instance_extensions);

    const app_info: vk.ApplicationInfo = .{
        .p_application_name = huge.name ++ " app",
        .application_version = @bitCast(@as(u32, 0)),
        .p_engine_name = huge.name,
        .engine_version = toVulkanVersion(huge.version),
        .api_version = toVulkanVersion(instance_api_version),
    };
    const instance_create_info: vk.InstanceCreateInfo = .{
        .p_application_info = &app_info,
        .enabled_extension_count = @intCast(instance_extensions.len),
        .pp_enabled_extension_names = instance_extensions.ptr,
        .enabled_layer_count = @intCast(layers.len),
        .pp_enabled_layer_names = layers.ptr,
    };
    const instance_handle = bwp.createInstance(&instance_create_info, vka) catch return VKError.InstanceInitializationFailure;

    iwp = .load(instance_handle, loader);
    instance = .init(instance_handle, &iwp);
}
fn initPhysicalDevices(allocator: Allocator, extensions: []const [*:0]const u8) VKError!void {
    var count: u32 = 0;
    _ = instance.enumeratePhysicalDevices(&count, null) catch
        return VKError.PhysicalDeviceInitializationFailure;

    if (count == 0) return error.PhysicalDeviceInitializationFailure;

    count = @min(physical_devices.len, count);
    var physical_device_handles: [physical_devices.len]vk.PhysicalDevice = undefined;
    _ = instance.enumeratePhysicalDevices(&count, &physical_device_handles) catch
        return VKError.PhysicalDeviceInitializationFailure;
    for (&physical_devices, &physical_device_handles) |*p, *ph| p.handle = ph.*;

    // create dummy window to use its surface
    // for physical device initialization
    const dummy_window = huge.Window.createDummy(@intFromEnum(instance.handle)) catch
        return VKError.DummyWindowCreationFailure;
    defer {
        instance.destroySurfaceKHR(@enumFromInt(dummy_window.surface_handle), vka);
        glfw.destroyWindow(dummy_window.handle);
    }

    valid_physical_device_count = count;
    var i: usize = 0;
    while (i < valid_physical_device_count) : (i += 1)
        initPhysicalDevice(allocator, &physical_devices[i], extensions, dummy_window) catch {
            valid_physical_device_count -= 1;
            std.mem.swap(PhysicalDevice, &physical_devices[i], &physical_devices[valid_physical_device_count]);
            i -= 1;
            continue; //remove from the array if initializaiton failed
        };
    if (valid_physical_device_count == 0) return VKError.PhysicalDeviceInitializationFailure;

    var max_score: u32 = 0; //pick best physical device
    //add ability to overwrite current physical device index
    for (physical_devices[0..valid_physical_device_count], 0..) |p, index| {
        const score = scorePhysicalDevice(p);
        if (score > max_score) {
            current_physical_device_index = index;
            max_score = score;
        }
    }
    if (false) std.debug.print("Format: {}\n", .{
        getVulkanFormat(.depth16_stencil8, .{
            .sampled = true,
            .sampled_linear = true,
            .sampled_minmax = true,
            // .color_attachment = true,
            // .color_attachment_blend = true,
            .depth_stencil_attachment = true,
            // .blit_src = true,
            // .blit_dst = true,
            // .transfer_src = true,
            // .transfer_dst = true,
        }, .image_optimal),
    });
}

fn scorePhysicalDevice(physical_device: PhysicalDevice) u32 {
    var score: u32 = 0;
    score = switch (physical_device.type) {
        .discrete_gpu => 4000,
        .integrated_gpu => 3000,
        .virtual_gpu => 2000,
        .cpu => 1000,
        else => 1,
    };
    if (physical_device.features.geometry_shaders) score += 100;
    if (physical_device.features.tessellation_shaders) score += 100;
    return score;
}

fn initPhysicalDevice(allocator: Allocator, p: *PhysicalDevice, extensions: []const [*:0]const u8, dummy_window: huge.Window.DummyWindow) VKError!void {
    p.features = getPhysicalDeviceFeatures(p.handle);

    const properties = instance.getPhysicalDeviceProperties(p.handle); //limits?
    if (!castVersion(@bitCast(properties.api_version)).@">="(min_vulkan_version))
        return VKError.UnsupportedApiVersion;

    p.type = properties.device_type;
    p.name_len = @min(
        std.mem.len(@as([*:0]const u8, @ptrCast(@alignCast(&properties.device_name)))),
        PhysicalDevice.max_name_len,
    );
    @memcpy(p.name_storage[0..p.name_len], properties.device_name[0..p.name_len]);

    const available_extensions =
        instance.enumerateDeviceExtensionPropertiesAlloc(p.handle, null, allocator) catch return VKError.OutOfMemory;
    try checkExtensionPresence(extensions, available_extensions);
    allocator.free(available_extensions);

    p.queue_family_indices =
        try getQueueFamilyIndices(allocator, p.handle, dummy_window);
}
fn getQueueFamilyIndices(allocator: Allocator, handle: vk.PhysicalDevice, dummy_window: huge.Window.DummyWindow) VKError![queue_type_count]u8 {
    const queue_family_properties = instance.getPhysicalDeviceQueueFamilyPropertiesAlloc(handle, allocator) catch
        return VKError.OutOfMemory;
    defer allocator.free(queue_family_properties);

    var index_lists: [queue_type_count]IndexList = undefined;

    for (&index_lists) |*l| l.init();

    for (queue_family_properties, 0..) |qfp, i| {
        const flags: QueueConfiguration = .{
            .graphics = qfp.queue_flags.graphics_bit,
            .compute = qfp.queue_flags.compute_bit,
            .transfer = qfp.queue_flags.transfer_bit,
            .sparse_binding = qfp.queue_flags.sparse_binding_bit,
            .protected = qfp.queue_flags.protected_bit,
            .video_decode = qfp.queue_flags.video_decode_bit_khr,
            .video_encode = qfp.queue_flags.video_encode_bit_khr,
            .presentation = @intFromEnum(instance.getPhysicalDeviceSurfaceSupportKHR(handle, @intCast(i), @enumFromInt(dummy_window.surface_handle)) catch .false) > 0,
        };
        inline for (@typeInfo(QueueType).@"enum".fields, 0..) |ef, j|
            if (@field(flags, ef.name))
                index_lists[j].append(@intCast(i));
    }
    //check for minimal reqired queues
    var any_flags: QueueConfiguration = .{};
    inline for (@typeInfo(QueueType).@"enum".fields, 0..) |ef, i| {
        // check if there are any queue families
        // at the index corresponding to that queue
        @field(any_flags, ef.name) = index_lists[i].list.items.len > 0;
    }
    if (!util.matchFlagStructs(
        QueueConfiguration,
        any_flags,
        minimal_required_queue_family_config,
    )) return VKError.MissingQueueType;

    //iterate through all the possible queue configurations score them and use the best one
    var non_empty_index_storage: [queue_type_count]usize = undefined;
    var count: usize = 0;
    //use this to avoid iterating through queue families that have no available queue
    for (&index_lists, 0..) |*l, i| {
        if (l.list.items.len > 0) {
            non_empty_index_storage[count] = i;
            count += 1;
        }
    }
    var max_score: i32 = std.math.minInt(i32);
    var current_queue_family_indices: [queue_type_count]u8 = @splat(0xff);

    var queue_family_indices: [queue_type_count]u8 = @splat(0xff);
    findBestQueueConfiguration(
        &queue_family_indices,
        index_lists,
        non_empty_index_storage[0..count],
        &current_queue_family_indices,
        0,
        &max_score,
    );
    return queue_family_indices;
}

const IndexList = struct {
    list: std.ArrayList(u8),
    buf: [max_queue_family_count]u8,
    pub fn init(self: *IndexList) void {
        self.list = .initBuffer(&self.buf);
    }
    pub fn append(self: *IndexList, i: u8) void {
        self.list.appendAssumeCapacity(i);
    }
};
fn findBestQueueConfiguration(
    out: *[queue_type_count]u8,
    index_lists: [queue_type_count]IndexList,
    non_empty_indices: []usize,
    current: *[queue_type_count]u8,
    depth: usize,
    max_score: *i32,
) void {
    if (depth == non_empty_indices.len) {
        const score = scoreQueueConfiguration(current);
        if (score > max_score.*) {
            max_score.* = score;
            //copy the best into the global storage
            out.* = current.*;
        }
        return;
    }
    const index = non_empty_indices[depth];
    for (index_lists[index].list.items) |value| {
        current[index] = value;
        findBestQueueConfiguration(
            out,
            index_lists,
            non_empty_indices,
            current,
            depth + 1,
            max_score,
        );
    }
}
fn scoreQueueConfiguration(configuration: []u8) i32 {
    var score: i32 = 0;
    inline for (queueConfigurationScoringRules) |rule| {
        const values: [2]u8 = .{
            configuration[@intFromEnum(rule[1][0])],
            configuration[@intFromEnum(rule[1][1])],
        };
        if (values[0] == values[1] and ~values[0] != 0) score += rule[0];
    }
    return score;
}
fn getPhysicalDeviceFeatures(handle: vk.PhysicalDevice) gpu.FeatureSet {
    const vk_features = instance.getPhysicalDeviceFeatures(handle);
    return .{
        .geometry_shaders = vk_features.geometry_shader != .false,
        .tessellation_shaders = vk_features.tessellation_shader != .false,
        .shader_float64 = vk_features.shader_float_64 != .false,
        .shader_int64 = vk_features.shader_int_64 != .false,
        .shader_int16 = vk_features.shader_int_16 != .false,
    };
}
const PhysicalDevice = struct {
    handle: vk.PhysicalDevice = .null_handle,
    queue_family_indices: [queue_type_count]u8 = @splat(0xff),
    name_storage: [max_name_len]u8 = @splat(0),
    name_len: usize = 0,
    features: gpu.FeatureSet = .{},
    type: vk.PhysicalDeviceType = .discrete_gpu,

    pub const max_name_len = 128;

    pub fn getUniqueQFIIndex(self: *const PhysicalDevice, queue_type: QueueType) usize {
        const qfi = self.queueFamilyIndex(queue_type);
        var unique_qfi: [queue_type_count]u8 = @splat(0xff);
        var count: usize = 0;

        return for (&self.queue_family_indices) |q| {
            if (q == qfi) break count;
            for (unique_qfi[0..count]) |u| (if (u == q) break) else {
                unique_qfi[count] = q;
                count += 1;
            }
        } else unreachable;
    }
    pub fn queueFamilyIndex(self: *const PhysicalDevice, queue_type: QueueType) u8 {
        return self.queue_family_indices[@intFromEnum(queue_type)];
    }
    pub fn format(self: PhysicalDevice, writer: *std.Io.Writer) !void {
        try writer.print("Physical Device({}){{\n", .{self.handle});
        try writer.print("Name: {s}\n", .{self.name_storage[0..self.name_len]});
        try writer.print("Family Queue Indices: {any}\n", .{self.queue_family_indices});
        try writer.print("Type: {}\n", .{self.type});
        try writer.print("Features: {{\n", .{});
        inline for (@typeInfo(gpu.Feature).@"enum".fields) |ef|
            try writer.print("\t{s} = {}\n", .{ ef.name, @field(self.features, ef.name) });
        try writer.print("}}", .{});
    }
};
fn checkExtensionPresence(required: []const [*:0]const u8, available: []const vk.ExtensionProperties) VKError!void {
    for (required) |re| { //chech for unavailable instance extensions
        if (!for (available) |ae| {
            if (huge.util.strEqlNullTerm(re, @ptrCast(@alignCast(&ae.extension_name)))) break true;
        } else false) return VKError.UnavailableExtension; //TODO: log missing extension name
    }
}
fn checkLayerPresence(required: []const [*:0]const u8, available: []const vk.LayerProperties) VKError!void {
    for (required) |rl| { //chech for unavailable instance extensions
        if (!for (available) |al| {
            if (huge.util.strEqlNullTerm(rl, @ptrCast(@alignCast(&al.layer_name)))) break true;
        } else false) return VKError.UnavailableLayer; //TODO: log missing layer name
    }
}

//=======================

pub const queue_type_count = util.enumLen(QueueType);
const QueueConfiguration = util.StructFromEnum(QueueType, bool, false, .@"packed");
const queueConfigurationScoringRules: []const std.meta.Tuple(&.{ i32, [2]QueueType }) = &.{
    .{ -150, .{ .graphics, .compute } },
    .{ -150, .{ .graphics, .transfer } },
    .{ 100, .{ .graphics, .presentation } },
    .{ -90, .{ .compute, .transfer } },
    .{ 30, .{ .sparse_binding, .transfer } },
};
pub const minimal_required_queue_family_config: QueueConfiguration = .{
    .graphics = true,
    .presentation = true,
    .transfer = true,
    .compute = true,
};
pub const QueueType = enum(u8) { graphics, presentation, compute, transfer, sparse_binding, protected, video_decode, video_encode };

//=======================
fn getDescriptorTypeHGSL(opaque_type: hgsl.OpaqueType) vk.DescriptorType {
    return switch (opaque_type) {
        .ubo => .uniform_buffer,
        .ssbo => .storage_buffer,
        .sampled_texture => .combined_image_sampler,
        .texture => .storage_image,
    };
}
fn getDescriptorType(opaque_type: gpu.OpaqueType, handle: gpu.Handle) vk.DescriptorType {
    return switch (opaque_type) {
        .buffer => blk: {
            const buffer = VKBuffer.get(@enumFromInt(handle));
            break :blk if (buffer.usage == .uniform) .uniform_buffer else .storage_buffer;
        },
        .texture => blk: {
            const texture = VKTexture.get(@enumFromInt(handle));
            break :blk if (texture.sampling_options == null) .storage_image else .combined_image_sampler;
        },
    };
}
fn getStageFlags(stage_info: hgsl.StageInfo) vk.ShaderStageFlags {
    return .{
        .vertex_bit = stage_info == .vertex,
        .fragment_bit = stage_info == .fragment,
    };
}
fn castPrimitiveTopology(primitive: gpu.PrimitiveTopology) vk.PrimitiveTopology {
    return switch (primitive) {
        .triangle => .triangle_list,
        .triangle_strip => .triangle_strip,
        .triangle_fan => .triangle_fan,
        .line => .line_list,
        .line_strip => .line_strip,
        .point => .point_list,
    };
}
pub const loader = &struct {
    pub fn l(i: vk.Instance, name: [*:0]const u8) ?glfw.VKproc {
        return glfw.getInstanceProcAddress(@intFromEnum(i), name);
    }
}.l;
fn castVersion(vk_version: vk.Version) Version {
    return .{
        .major = vk_version.major,
        .minor = vk_version.minor,
    };
}
fn toVulkanVersion(version: Version) u32 {
    return @bitCast(vk.makeApiVersion(0, @truncate(version.major), @truncate(version.minor), 0));
}
const glfw = huge.Window.glfw;
const Error = gpu.Error;
const Pipeline = gpu.Pipeline;
const Format = gpu.Format;
const ShaderModule = gpu.ShaderModule;
const RenderTarget = gpu.RenderTarget;
const Texture = gpu.Texture;
const TextureDimensions = gpu.TextureDimensions;
const Buffer = gpu.Buffer;
const BufferUsage = gpu.BufferUsage;
const WindowContext = gpu.WindowContext;
const ClearValue = gpu.ClearValue;
const Version = huge.Version;
const Allocator = std.mem.Allocator;
const List = std.ArrayList;
pub const FormatUsage = packed struct(u11) {
    vertex: bool = false,
    sampled: bool = false,
    sampled_linear: bool = false,
    sampled_minmax: bool = false,

    color_attachment: bool = false,
    color_attachment_blend: bool = false,
    depth_stencil_attachment: bool = false,

    blit_src: bool = false,
    blit_dst: bool = false,

    transfer_src: bool = false,
    transfer_dst: bool = false,
    //video decode/encode
    pub fn toImageUsage(self: FormatUsage) vk.ImageUsageFlags {
        return .{
            .transfer_src_bit = self.transfer_src,
            .transfer_dst_bit = self.transfer_dst,
            .sampled_bit = self.sampled,
            .storage_bit = false, // !
            .color_attachment_bit = self.color_attachment,
            .depth_stencil_attachment_bit = self.depth_stencil_attachment,
        };
    }
};
fn formatFromIOType(io_type: hgsl.IOType) vk.Format {
    return if (io_type == .scalar) switch (io_type.scalar) {
        .f32 => .r32_sfloat,
        .i32 => .r32_sint,
        .u32 => .r32_uint,
        .f64 => .r64_sfloat,
        .i64 => .r64_sint,
        .u64 => .r64_uint,
    } else switch (io_type.vector.len) {
        ._2 => switch (io_type.vector.component) {
            .f32 => .r32g32_sfloat,
            .i32 => .r32g32_sint,
            .u32 => .r32g32_uint,
            .f64 => .r64g64_sfloat,
            .i64 => .r64g64_sint,
            .u64 => .r64g64_uint,
        },
        ._3 => switch (io_type.vector.component) {
            .f32 => .r32g32b32_sfloat,
            .i32 => .r32g32b32_sint,
            .u32 => .r32g32b32_uint,
            .f64 => .r64g64b64_sfloat,
            .i64 => .r64g64b64_sint,
            .u64 => .r64g64b64_uint,
        },
        ._4 => switch (io_type.vector.component) {
            .f32 => .r32g32b32a32_sfloat,
            .i32 => .r32g32b32a32_sint,
            .u32 => .r32g32b32a32_uint,
            .f64 => .r64g64b64a64_sfloat,
            .i64 => .r64g64b64a64_sint,
            .u64 => .r64g64b64a64_uint,
        },
    };
}

const FormatUsageLayout = enum {
    image_linear,
    image_optimal,
    buffer,
};
pub fn getVulkanFormat(format: Format, usage: FormatUsage, layout: FormatUsageLayout) vk.Format {
    return for (frmt.compatibleFormats(format)) |vkf| {
        const properties = instance.getPhysicalDeviceFormatProperties(pd().handle, vkf);
        const flags: vk.FormatFeatureFlags = switch (layout) {
            .image_linear => properties.linear_tiling_features,
            .image_optimal => properties.optimal_tiling_features,
            .buffer => properties.buffer_features,
        };
        const valid = (!usage.vertex or flags.vertex_buffer_bit) and
            (!usage.sampled or flags.sampled_image_bit) and
            (!usage.sampled_linear or flags.sampled_image_filter_linear_bit) and
            (!usage.sampled_minmax or flags.sampled_image_filter_minmax_bit) and
            (!usage.color_attachment or flags.color_attachment_bit) and
            (!usage.color_attachment_blend or flags.color_attachment_blend_bit) and
            (!usage.depth_stencil_attachment or flags.depth_stencil_attachment_bit) and
            (!usage.blit_src or flags.blit_src_bit) and
            (!usage.blit_dst or flags.blit_dst_bit) and
            (!usage.transfer_src or flags.transfer_src_bit) and
            (!usage.transfer_dst or flags.transfer_dst_bit);
        if (valid) break vkf;
    } else .undefined;
}
const VKError = error{
    OutOfMemory,

    UnavailableExtension,
    UnsupportedApiVersion,
    UnavailableLayer,

    InstanceInitializationFailure,
    PhysicalDeviceInitializationFailure,
    DummyWindowCreationFailure,
    MissingQueueType,

    LogicalDeviceInitializationFailure,

    ResourceCreationError,
};
fn versionBackend(version: Version) gpu.Backend {
    return .{
        // .api = .vulkan,
        .api_version = version,
        // .deinit = &deinit,

        // .draw = &draw,
        // .bindVertexBuffer = &bindVertexBuffer,
        // .bindIndexBuffer = &bindIndexBuffer,
        // .pipelinePushConstant = &pipelinePushConstant,
        // .pipelineSetOpaqueUniform = &pipelineSetOpaqueUniform,

        // .beginRendering = &beginRendering,
        // .endRendering = &endRendering,

        // .reloadPipelines = &reloadPipelines,
        // .createPipeline = &createPipeline,
        // .createShaderModulePath = &createShaderModulePath,
        // .destroyShaderModule = &destroyShaderModule,

        // .renderTargetSize = &renderTargetSize,
        // .createRenderTargetFromTextures = &createRenderTargetFromTextures,
        // .createRenderTarget = &createRenderTarget,
        // .destroyRenderTarget = &destroyRenderTarget,

        // .getTextureDimensions = &getTextureDimensions,
        // .getTextureFormat = &getTextureFormat,
        // .createTexture = &createTexture,
        // .destroyTexture = &destroyTexture,

        // .getBufferUsage = &getBufferUsage,
        // .loadBuffer = &loadBuffer,
        // .mapBuffer = &mapBuffer,
        // .unmapBuffer = &unmapBuffer,
        // .createBuffer = &createBuffer,
        // .destroyBuffer = &destroyBuffer,

        // .updateWindowContext = &updateWindowContext,
        // .getWindowRenderTarget = &getWindowRenderTarget,
        // .createWindowContext = &createWindowContext,
        // .destroyWindowContext = &destroyWindowContext,
    };
}
